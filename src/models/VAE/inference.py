import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
from tqdm import tqdm
import os

from src.data.preprocessor import DataProcessor
from src.data.dataset import StaticDataset
from src.models.multi_vae import MultiVAE
from src.loss.loss import multivae_loss

CONFIG = {
    # ìµœì ì—í­
    'best_epoch': 30,
    
    # ëª¨ë¸ í•˜ì´í¼ íŒŒë¼ë¯¸í„°
    'hidden_dim': 2048,
    'latent_dim': 256,
    'dropout_rate': 0.5,
    'lr': 5e-4,
    'batch_size': 500,
    'k': 10,
    
    # Annealing ì„¤ì •
    'anneal_cap': 0.2,
    # 'anneal_ratio': 0.7,
    'total_anneal_steps' : 30000,
    
    # ê²½ë¡œ ì„¤ì •
    'data_path': './data/train/train_ratings.csv',
    'output_path': '../output/multi_vae_submission_2.csv'
}

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def main():
    print(f"ğŸš€ Starting Inference Pipeline on {device}")
    print(f"ğŸ¯ Target Epoch: {CONFIG['best_epoch']} (Full Training)")
    
    
    # ==========================================
    # 1. Data Loading & Processing (Full Data)
    # ==========================================
    print("\n[Step 1] Loading  Processing Data...")
    processor = DataProcessor(CONFIG['data_path'])
    
    # Split ì—†ì´ ì „ì²´ ë°ì´í„° ë¡œë“œ ë° ì¸ì½”ë”©
    df = processor.load_and_process()
    
    # ì „ì²´ ë°ì´í„°ë¥¼ CSR Matrixë¡œ ë³€í™˜
    full_matrix = processor._create_csr_matrix(df)
    print(f"    Data Shape: {full_matrix.shape} (Users: {full_matrix.shape[0]}, Items: {full_matrix.shape[1]})")
    
    # Dataset ìƒì„±
    full_dataset = StaticDataset(full_matrix)
    
    # Loader ìƒì„±
    # í•™ìŠµìš© Loader(Shuffle=True)
    train_loader = DataLoader(full_dataset, batch_size=CONFIG['batch_size'], shuffle=True)
    
    # ì¶”ë¡ ìš© Loader(Shuffle=False) ìˆœì„œì§€ì¼œì•¼í•¨
    inference_loader = DataLoader(full_dataset, batch_size=CONFIG['batch_size'], shuffle=False)
    
    
    # ==========================================
    # 2. Model & Optimizer Initialization
    # ==========================================
    print("\n[Step 2] Initializing Model...")
    input_dim = full_matrix.shape[1]
    p_dims = [input_dim, CONFIG['hidden_dim'], CONFIG['latent_dim']]
    
    model = MultiVAE(p_dims=p_dims, dropout_rate=CONFIG['dropout_rate']).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['lr'])
    
    
    # ==========================================
    # 3. Full Training Loop
    # ==========================================
    print("\n[Step 3] Re-training on Full Data...")
    model.train()
    
    # Annealing Step ìë™ ê³„ì‚° (ë°°ì¹˜ ìˆ˜ * ì—í­ * ë¹„ìœ¨)
    total_steps = CONFIG['total_anneal_steps'] #len(train_loader) * CONFIG['best_epoch'] * CONFIG['anneal_ratio']
    update_count = 0
    
    for epoch in range(1, CONFIG['best_epoch'] + 1):
        total_loss = 0
        pbar = tqdm(train_loader, desc=f"Ep {epoch}/{CONFIG['best_epoch']}", leave=False)
        
        for batch in pbar:
            batch = batch.to(device)
            optimizer.zero_grad()
            
            # Forward
            recon_batch, mu, logvar = model(batch)
            
            # Anneal Logic
            if total_steps > 0:
                anneal = min(CONFIG['anneal_cap'], 1.0 * update_count / total_steps)
            else:
                anneal = CONFIG['anneal_cap']
            update_count += 1
            
            # Loss & Backward
            loss = multivae_loss(recon_batch, batch, mu, logvar, anneal)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            pbar.set_postfix({'loss': loss.item(), 'anneal': anneal})
        
        print(f"    Epoch {epoch:03d} | Loss: {total_loss/len(train_loader):.4f}")
    
    print("     >>> Full Training Finished!")
    
    
    # ==========================================
    # 4. Inference & Submission Generation
    # ==========================================
    print("\n[Step 4] Generating Submission File...")
    model.eval()
    
    all_users = []
    all_items = []
    
    # User ID ë³µì›ì„ ìœ„í•œ ì‹œì‘ ì¸ë±ìŠ¤
    user_start_idx = 0
    
    with torch.no_grad():
        for batch_input in tqdm(inference_loader, desc="Inferencing"):
            
            batch_input = batch_input.to(device)
            
            # ì˜ˆì¸¡
            recon_batch, _, _ = model(batch_input)
            
            # ì´ë¯¸ ë³¸ ì•„ì´í…œ ë§ˆìŠ¤í‚¹
            recon_batch[batch_input.nonzero(as_tuple=True)] = -float('inf')
            
            # Top-K ì„ ì •
            _, topk_indices = torch.topk(recon_batch, k=CONFIG['k'], dim=1)
            
            # CPU & Numpy ë³€í™˜
            topk_indices = topk_indices.cpu().numpy()
            
            # ID Decoding
            batch_size = batch_input.size(0)
            
            # í˜„ì¬ ë°°ì¹˜ì˜ User Indexë“¤
            current_user_indices = np.arange(user_start_idx, user_start_idx + batch_size)
            
            # User ID ë³µì›
            decoded_users = processor.user_encoder.inverse_transform(current_user_indices)
            
            # Item ID ë³µì› ë° ì €ì¥
            for i in range(batch_size):
                rec_item_indices = topk_indices[i]
                rec_item_ids = processor.item_encoder.inverse_transform(rec_item_indices)
                
                for item_id in rec_item_ids:
                    all_users.append(decoded_users[i])
                    all_items.append(item_id)
            
            user_start_idx += batch_size
    
    # ==========================================
    # 5. Save to CSV
    # ==========================================
    submission = pd.DataFrame({
        'user': all_users,
        'item': all_items
    })
    
    submission.to_csv(CONFIG['output_path'], index=False)
    print(f"\nâœ… Submission Saved Successfully: {CONFIG['output_path']}")
    print(f"   Total Rows: {len(submission)}")

if __name__ == '__main__':
    main()
    
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-VAE (Collaborative Filtering) Visualization\n",
    "\n",
    "이 노트북은 학습된 Multi-VAE 모델과 추천 시스템 데이터를 시각화합니다:\n",
    "1. Input Data 시각화: User-Item Matrix\n",
    "2. Data Split 시각화: 4가지 분할 전략 비교\n",
    "3. Prediction 시각화: Top-10 추천 및 Recall@K\n",
    "4. 잠재 공간 시각화: 3D Multivariate Normal Distribution\n",
    "5. 출력 분포 시각화: Multinomial Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf\n",
    "import logging\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.models.multi_vae import MultiVAE\n",
    "from src.data.recsys_data import RecSysDataModule\n",
    "from src.utils.recommend import recommend_topk\n",
    "from src.utils.metrics import recall_at_k\n",
    "\n",
    "# 시각화 설정\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# 한글폰트 : apt-get install -y fonts-nanum\n",
    "plt.rc('font', family='NanumGothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "# 모델 이름 (Hydra 로그 디렉토리 구조)\n",
    "MODEL_NAME = \"multi-vae\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_timestamp 설정 (None이면 가장 최근 실행 자동 선택)\n",
    "run_timestamp = None  # 예: \"2025-12-20/16-44-09\"\n",
    "\n",
    "# 가장 최근 실행 찾기\n",
    "def find_latest_run(base_dir=f\"../saved/hydra_logs/{MODEL_NAME}\"):\n",
    "    \"\"\"가장 최근 실행 디렉토리 찾기\"\"\"\n",
    "    pattern = os.path.join(base_dir, \"*\", \"*\")\n",
    "    all_runs = glob.glob(pattern)\n",
    "    \n",
    "    if not all_runs:\n",
    "        raise FileNotFoundError(f\"No run directories found in {base_dir}\")\n",
    "    \n",
    "    # 날짜/시간 기준으로 정렬\n",
    "    all_runs.sort()\n",
    "    latest_run = all_runs[-1]\n",
    "    \n",
    "    # 상대 경로로 변환: saved/hydra_logs/2025-12-20/16-44-09 -> 2025-12-20/16-44-09\n",
    "    run_timestamp = os.path.relpath(latest_run, base_dir)\n",
    "    \n",
    "    return run_timestamp\n",
    "\n",
    "if run_timestamp is None:\n",
    "    run_timestamp = find_latest_run()\n",
    "    print(f\"✅ Using latest run: {run_timestamp}\")\n",
    "else:\n",
    "    print(f\"✅ Using specified run: {run_timestamp}\")\n",
    "\n",
    "# Hydra 설정 로드\n",
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "    cfg = compose(config_name=\"multi_vae\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Configuration:\")\n",
    "print(OmegaConf.to_yaml(cfg))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "data_module = RecSysDataModule(\n",
    "    data_dir=cfg.data.data_dir,\n",
    "    batch_size=cfg.data.batch_size,\n",
    "    valid_ratio=cfg.data.valid_ratio,\n",
    "    min_interactions=cfg.data.min_interactions,\n",
    "    seed=cfg.seed,\n",
    "    data_file=cfg.data.data_file,\n",
    "    split_strategy=cfg.data.split_strategy,\n",
    "    temporal_split_ratio=cfg.data.get(\"temporal_split_ratio\", 0.8),\n",
    ")\n",
    "data_module.setup()\n",
    "\n",
    "print(f\"Number of users: {data_module.num_users}\")\n",
    "print(f\"Number of items: {data_module.num_items}\")\n",
    "print(f\"Split strategy: {data_module.split_strategy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 로드 (Hydra 실행 디렉토리 기준)\n",
    "checkpoint_dir = os.path.join('..', 'saved', 'hydra_logs', MODEL_NAME, run_timestamp, 'checkpoints')\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    raise FileNotFoundError(f\"Checkpoint directory not found: {checkpoint_dir}\")\n",
    "\n",
    "# 체크포인트 파일 찾기\n",
    "checkpoints = glob.glob(os.path.join(checkpoint_dir, \"multi-vae-*.ckpt\"))\n",
    "\n",
    "if not checkpoints:\n",
    "    raise FileNotFoundError(f\"No checkpoints found in {checkpoint_dir}\")\n",
    "\n",
    "# 가장 좋은 체크포인트 선택 (val_loss 기준)\n",
    "# 파일명: multi-vae-epoch=XX-val_loss=YY.YYYY.ckpt\n",
    "def extract_val_loss(ckpt_path):\n",
    "    filename = os.path.basename(ckpt_path)\n",
    "    try:\n",
    "        loss_str = filename.split(\"val_loss=\")[1].split(\".ckpt\")[0]\n",
    "        return float(loss_str)\n",
    "    except:\n",
    "        return float('inf')\n",
    "\n",
    "best_checkpoint = min(checkpoints, key=extract_val_loss)\n",
    "print(f\"✅ Loading checkpoint: {best_checkpoint}\")\n",
    "\n",
    "# 모델 로드\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MultiVAE.load_from_checkpoint(\n",
    "    best_checkpoint,\n",
    "    num_items=data_module.num_items,\n",
    "    weights_only=False\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"✅ Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Input Data 시각화: User-Item Matrix\n",
    "\n",
    "User-Item 상호작용 행렬을 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_user_item_matrix(train_mat, num_users=100, num_items=200):\n",
    "    \"\"\"\n",
    "    User-Item 행렬 시각화 (샘플링)\n",
    "    \"\"\"\n",
    "    # 샘플링 (너무 크면 일부만)\n",
    "    sample_users = min(num_users, train_mat.shape[0])\n",
    "    sample_items = min(num_items, train_mat.shape[1])\n",
    "    \n",
    "    # Dense로 변환 (샘플만)\n",
    "    sample_mat = train_mat[:sample_users, :sample_items].toarray()\n",
    "    \n",
    "    # 시각화\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # 1. Heatmap\n",
    "    sns.heatmap(sample_mat, cmap='YlOrRd', cbar_kws={'label': 'Interaction'},\n",
    "                ax=axes[0], xticklabels=False, yticklabels=False)\n",
    "    axes[0].set_title(f'User-Item Matrix (Sample: {sample_users} users × {sample_items} items)', fontsize=14)\n",
    "    axes[0].set_xlabel('Items', fontsize=12)\n",
    "    axes[0].set_ylabel('Users', fontsize=12)\n",
    "    \n",
    "    # 2. 통계\n",
    "    total_interactions = train_mat.nnz\n",
    "    total_possible = train_mat.shape[0] * train_mat.shape[1]\n",
    "    sparsity = 100 * (1 - total_interactions / total_possible)\n",
    "    \n",
    "    interactions_per_user = np.array(train_mat.sum(axis=1)).flatten()\n",
    "    interactions_per_item = np.array(train_mat.sum(axis=0)).flatten()\n",
    "    \n",
    "    axes[1].hist(interactions_per_user, bins=50, alpha=0.6, label='Per User', edgecolor='black')\n",
    "    axes[1].axvline(interactions_per_user.mean(), color='red', linestyle='--', \n",
    "                    label=f'Mean: {interactions_per_user.mean():.1f}')\n",
    "    axes[1].set_xlabel('Number of Interactions', fontsize=12)\n",
    "    axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[1].set_title('Interaction Distribution per User', fontsize=14)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 통계 출력\n",
    "    print(f\"\\n=== User-Item Matrix Statistics ===\")\n",
    "    print(f\"Total Users: {train_mat.shape[0]:,}\")\n",
    "    print(f\"Total Items: {train_mat.shape[1]:,}\")\n",
    "    print(f\"Total Interactions: {total_interactions:,}\")\n",
    "    print(f\"Matrix Sparsity: {sparsity:.2f}%\")\n",
    "    print(f\"\\nInteractions per User: mean={interactions_per_user.mean():.1f}, median={np.median(interactions_per_user):.1f}\")\n",
    "    print(f\"Interactions per Item: mean={interactions_per_item.mean():.1f}, median={np.median(interactions_per_item):.1f}\")\n",
    "\n",
    "train_mat = data_module.get_train_matrix()\n",
    "visualize_user_item_matrix(train_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Split 시각화: 4가지 분할 전략\n",
    "\n",
    "Random, Leave-One-Out, Temporal User, Temporal Global 분할 전략을 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_split_strategies():\n",
    "    \"\"\"\n",
    "    4가지 split 전략을 시각적으로 비교\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Data Split Strategies Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 샘플 유저 데이터 (시각화용)\n",
    "    user_items = np.array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19])  # 10개 아이템\n",
    "    timestamps = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  # 시간 순서\n",
    "    \n",
    "    # 1. Random Split\n",
    "    ax = axes[0, 0]\n",
    "    np.random.seed(42)\n",
    "    valid_idx = np.random.choice(len(user_items), size=2, replace=False)\n",
    "    train_mask = np.ones(len(user_items), dtype=bool)\n",
    "    train_mask[valid_idx] = False\n",
    "    \n",
    "    ax.scatter(np.arange(len(user_items))[train_mask], user_items[train_mask], \n",
    "               c='blue', s=100, label='Train', marker='o')\n",
    "    ax.scatter(np.arange(len(user_items))[~train_mask], user_items[~train_mask], \n",
    "               c='red', s=100, label='Validation', marker='s')\n",
    "    ax.set_title('1. Random Split\\n(유저별 랜덤 20% validation)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Interaction Index', fontsize=10)\n",
    "    ax.set_ylabel('Item ID', fontsize=10)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.text(0.5, -0.15, '장점: 시간 정보 불필요, Cold-start 시뮬레이션\\n단점: 시간적 패턴 무시',\n",
    "            ha='center', va='top', transform=ax.transAxes, fontsize=9, style='italic')\n",
    "    \n",
    "    # 2. Leave-One-Out\n",
    "    ax = axes[0, 1]\n",
    "    train_mask = np.ones(len(user_items), dtype=bool)\n",
    "    train_mask[-1] = False  # 마지막 1개만 validation\n",
    "    \n",
    "    ax.scatter(np.arange(len(user_items))[train_mask], user_items[train_mask], \n",
    "               c='blue', s=100, label='Train', marker='o')\n",
    "    ax.scatter(np.arange(len(user_items))[~train_mask], user_items[~train_mask], \n",
    "               c='red', s=100, label='Validation', marker='s')\n",
    "    ax.set_title('2. Leave-One-Out\\n(유저별 랜덤 1개만 validation)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Interaction Index', fontsize=10)\n",
    "    ax.set_ylabel('Item ID', fontsize=10)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.text(0.5, -0.15, '장점: 빠른 평가, 표준적 방법\\n단점: Validation set 작음, 분산 클 수 있음',\n",
    "            ha='center', va='top', transform=ax.transAxes, fontsize=9, style='italic')\n",
    "    \n",
    "    # 3. Temporal User Split\n",
    "    ax = axes[1, 0]\n",
    "    split_idx = int(len(user_items) * 0.8)  # 80% train\n",
    "    train_mask = np.zeros(len(user_items), dtype=bool)\n",
    "    train_mask[:split_idx] = True\n",
    "    \n",
    "    ax.scatter(timestamps[train_mask], user_items[train_mask], \n",
    "               c='blue', s=100, label='Train', marker='o')\n",
    "    ax.scatter(timestamps[~train_mask], user_items[~train_mask], \n",
    "               c='red', s=100, label='Validation', marker='s')\n",
    "    ax.axvline(x=timestamps[split_idx-1], color='green', linestyle='--', \n",
    "               linewidth=2, label='Split Point (80%)')\n",
    "    ax.set_title('3. Temporal User Split\\n(유저별 시간순 80% train, 20% valid)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Time', fontsize=10)\n",
    "    ax.set_ylabel('Item ID', fontsize=10)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.text(0.5, -0.15, '장점: 유저별 시간 패턴, 모든 유저 포함\\n단점: 유저 간 시간 섞임',\n",
    "            ha='center', va='top', transform=ax.transAxes, fontsize=9, style='italic')\n",
    "    \n",
    "    # 4. Temporal Global Split\n",
    "    ax = axes[1, 1]\n",
    "    # Global timeline 시뮬레이션\n",
    "    global_time = np.array([1, 3, 4, 6, 8, 10, 12, 15, 18, 20])  # 전역 시간\n",
    "    split_time = global_time[int(len(global_time) * 0.8)]  # 80% 지점\n",
    "    train_mask = global_time < split_time\n",
    "    \n",
    "    ax.scatter(global_time[train_mask], user_items[train_mask], \n",
    "               c='blue', s=100, label='Train', marker='o')\n",
    "    ax.scatter(global_time[~train_mask], user_items[~train_mask], \n",
    "               c='red', s=100, label='Validation', marker='s')\n",
    "    ax.axvline(x=split_time, color='green', linestyle='--', \n",
    "               linewidth=2, label=f'Global Split (t={split_time})')\n",
    "    ax.set_title('4. Temporal Global Split\\n(전역 시간 기준 80% train, 20% valid)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Global Time', fontsize=10)\n",
    "    ax.set_ylabel('Item ID', fontsize=10)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.text(0.5, -0.15, '장점: 진짜 시간 일반화, 실제 환경과 유사\\n단점: Cold-start 발생 가능',\n",
    "            ha='center', va='top', transform=ax.transAxes, fontsize=9, style='italic')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_split_strategies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 사용 중인 split strategy 통계\n",
    "valid_gt = data_module.get_validation_ground_truth()\n",
    "valid_counts = [len(items) for items in valid_gt.values()]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(valid_counts, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(np.mean(valid_counts), color='red', linestyle='--', \n",
    "            label=f'Mean: {np.mean(valid_counts):.1f}')\n",
    "plt.xlabel('# Validation Items per User', fontsize=11)\n",
    "plt.ylabel('Frequency', fontsize=11)\n",
    "plt.title(f'Validation Set Distribution\\n(Strategy: {data_module.split_strategy})', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "stats_data = {\n",
    "    'Metric': ['Total Users', 'Total Items', 'Train Interactions', 'Valid Interactions', 'Sparsity (%)'],\n",
    "    'Value': [\n",
    "        f\"{data_module.num_users:,}\",\n",
    "        f\"{data_module.num_items:,}\",\n",
    "        f\"{train_mat.nnz:,}\",\n",
    "        f\"{sum(valid_counts):,}\",\n",
    "        f\"{100 * (1 - train_mat.nnz / (data_module.num_users * data_module.num_items)):.2f}\"\n",
    "    ]\n",
    "}\n",
    "df_stats = pd.DataFrame(stats_data)\n",
    "plt.axis('off')\n",
    "table = plt.table(cellText=df_stats.values, colLabels=df_stats.columns,\n",
    "                  cellLoc='left', loc='center', colWidths=[0.6, 0.4])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "plt.title(f'Dataset Statistics\\n(Split: {data_module.split_strategy})', fontsize=12, pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction 시각화: Top-10 추천 및 Recall@K\n",
    "\n",
    "모델의 Top-K 추천 결과와 평가 메트릭을 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-K 추천 생성\n",
    "print(\"Generating Top-K recommendations...\")\n",
    "recommendations = recommend_topk(\n",
    "    model,\n",
    "    train_mat,\n",
    "    k=cfg.recommend.topk,\n",
    "    device=device,\n",
    "    batch_size=cfg.data.batch_size\n",
    ")\n",
    "\n",
    "# Recall@K 계산\n",
    "valid_gt_list = [valid_gt[u] for u in range(data_module.num_users)]\n",
    "pred_list = [rec.tolist() for rec in recommendations]\n",
    "recall = recall_at_k(valid_gt_list, pred_list, k=cfg.recommend.topk)\n",
    "\n",
    "print(f\"\\nValidation Recall@{cfg.recommend.topk}: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_recommendations(user_idx, train_mat, recommendations, valid_gt, num_show=10):\n",
    "    \"\"\"\n",
    "    특정 유저의 추천 결과 시각화\n",
    "    \"\"\"\n",
    "    # 유저 데이터\n",
    "    user_train = train_mat[user_idx].toarray().flatten()\n",
    "    user_train_items = np.where(user_train > 0)[0]\n",
    "    user_recs = recommendations[user_idx]\n",
    "    user_valid = valid_gt[user_idx]\n",
    "    \n",
    "    # Hit 계산\n",
    "    hits = [item in user_valid for item in user_recs]\n",
    "    n_hits = sum(hits)\n",
    "    user_recall = n_hits / len(user_valid) if len(user_valid) > 0 else 0\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # 1. Train items\n",
    "    ax = axes[0]\n",
    "    if len(user_train_items) > num_show:\n",
    "        sample_train = np.random.choice(user_train_items, num_show, replace=False)\n",
    "    else:\n",
    "        sample_train = user_train_items\n",
    "    \n",
    "    ax.barh(range(len(sample_train)), sample_train, color='steelblue')\n",
    "    ax.set_yticks(range(len(sample_train)))\n",
    "    ax.set_yticklabels([f'Item {i}' for i in range(len(sample_train))])\n",
    "    ax.set_xlabel('Item ID', fontsize=11)\n",
    "    ax.set_title(f'Train Items (샘플 {len(sample_train)}/{len(user_train_items)})', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 2. Recommendations\n",
    "    ax = axes[1]\n",
    "    colors = ['green' if hit else 'gray' for hit in hits]\n",
    "    ax.barh(range(len(user_recs)), user_recs, color=colors)\n",
    "    ax.set_yticks(range(len(user_recs)))\n",
    "    ax.set_yticklabels([f'Rank {i+1}' for i in range(len(user_recs))])\n",
    "    ax.set_xlabel('Item ID', fontsize=11)\n",
    "    ax.set_title(f'Top-{len(user_recs)} Recommendations\\n(Green: Hit, Gray: Miss)', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 3. Validation GT\n",
    "    ax = axes[2]\n",
    "    if len(user_valid) > 0:\n",
    "        ax.barh(range(len(user_valid)), user_valid, color='orange')\n",
    "        ax.set_yticks(range(len(user_valid)))\n",
    "        ax.set_yticklabels([f'Item {i+1}' for i in range(len(user_valid))])\n",
    "        ax.set_xlabel('Item ID', fontsize=11)\n",
    "        ax.set_title(f'Validation Ground Truth\\n({len(user_valid)} items)', fontsize=12)\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No validation items', ha='center', va='center',\n",
    "                transform=ax.transAxes, fontsize=14)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    fig.suptitle(f'User {user_idx} - Recall@{len(user_recs)}: {user_recall:.2f} ({n_hits}/{len(user_valid)} hits)',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 여러 유저 샘플 시각화\n",
    "sample_users = np.random.choice(data_module.num_users, size=3, replace=False)\n",
    "for user_idx in sample_users:\n",
    "    visualize_recommendations(user_idx, train_mat, recommendations, valid_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall@K 분포 시각화\n",
    "user_recalls = []\n",
    "for u in range(data_module.num_users):\n",
    "    if len(valid_gt[u]) > 0:\n",
    "        hits = len(set(recommendations[u].tolist()) & set(valid_gt[u]))\n",
    "        user_recall = hits / min(cfg.recommend.topk, len(valid_gt[u]))\n",
    "        user_recalls.append(user_recall)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(user_recalls, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "plt.axvline(np.mean(user_recalls), color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Mean: {np.mean(user_recalls):.4f}')\n",
    "plt.axvline(np.median(user_recalls), color='green', linestyle='--', linewidth=2,\n",
    "            label=f'Median: {np.median(user_recalls):.4f}')\n",
    "plt.xlabel(f'Recall@{cfg.recommend.topk}', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title(f'Per-User Recall@{cfg.recommend.topk} Distribution', fontsize=13)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "recall_values = []\n",
    "k_values = [1, 5, 10, 20, 50]\n",
    "for k in k_values:\n",
    "    if k <= cfg.recommend.topk:\n",
    "        pred_k = [rec[:k].tolist() for rec in recommendations]\n",
    "        recall_k = recall_at_k(valid_gt_list, pred_k, k=k)\n",
    "        recall_values.append(recall_k)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "valid_k = k_values[:len(recall_values)]\n",
    "plt.plot(valid_k, recall_values, marker='o', linewidth=2, markersize=8, color='coral')\n",
    "plt.xlabel('K (Top-K)', fontsize=12)\n",
    "plt.ylabel('Recall@K', fontsize=12)\n",
    "plt.title('Recall@K vs K', fontsize=13)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(valid_k)\n",
    "\n",
    "for i, (k, r) in enumerate(zip(valid_k, recall_values)):\n",
    "    plt.text(k, r + 0.01, f'{r:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 잠재 공간 시각화: 3D Multivariate Normal Distribution\n",
    "\n",
    "유저들의 잠재 표현(latent representation)을 시각화하고, Multivariate Normal Distribution을 3차원으로 표현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유저 잠재 벡터 추출\n",
    "print(\"Extracting user latent representations...\")\n",
    "train_dense = torch.FloatTensor(train_mat.toarray()).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    mu, logvar = model.encode(train_dense)\n",
    "    mu = mu.cpu().numpy()\n",
    "    logvar = logvar.cpu().numpy()\n",
    "    std = np.exp(0.5 * logvar)\n",
    "\n",
    "latent_dim = mu.shape[1]\n",
    "print(f\"Latent dimension: {latent_dim}\")\n",
    "print(f\"Number of users: {mu.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3차원 시각화 (처음 3개 차원 사용)\n",
    "if latent_dim >= 3:\n",
    "    fig = plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # 1. 유저별 mu 분포 (3D scatter)\n",
    "    ax1 = fig.add_subplot(131, projection='3d')\n",
    "    \n",
    "    # 상호작용 개수로 색상 구분\n",
    "    n_interactions = np.array(train_mat.sum(axis=1)).flatten()\n",
    "    scatter = ax1.scatter(mu[:, 0], mu[:, 1], mu[:, 2], \n",
    "                         c=n_interactions, cmap='viridis', s=10, alpha=0.6)\n",
    "    ax1.set_xlabel('Latent Dim 1', fontsize=10)\n",
    "    ax1.set_ylabel('Latent Dim 2', fontsize=10)\n",
    "    ax1.set_zlabel('Latent Dim 3', fontsize=10)\n",
    "    ax1.set_title('User Latent Representations (μ)\\n(Color: # Interactions)', fontsize=12)\n",
    "    plt.colorbar(scatter, ax=ax1, shrink=0.5, label='# Interactions')\n",
    "    \n",
    "    # 2. Multivariate Normal Distribution (등고선)\n",
    "    ax2 = fig.add_subplot(132, projection='3d')\n",
    "    \n",
    "    # 전체 분포의 평균과 공분산\n",
    "    mean_mu = mu[:, :3].mean(axis=0)\n",
    "    cov_mu = np.cov(mu[:, :3].T)\n",
    "    \n",
    "    # Grid 생성\n",
    "    x = np.linspace(mu[:, 0].min(), mu[:, 0].max(), 30)\n",
    "    y = np.linspace(mu[:, 1].min(), mu[:, 1].max(), 30)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # 2D marginal distribution (dim 0, 1)\n",
    "    pos = np.dstack((X, Y))\n",
    "    rv = multivariate_normal(mean_mu[:2], cov_mu[:2, :2])\n",
    "    Z = rv.pdf(pos)\n",
    "    \n",
    "    ax2.plot_surface(X, Y, Z, cmap='coolwarm', alpha=0.7, edgecolor='none')\n",
    "    ax2.set_xlabel('Latent Dim 1', fontsize=10)\n",
    "    ax2.set_ylabel('Latent Dim 2', fontsize=10)\n",
    "    ax2.set_zlabel('Probability Density', fontsize=10)\n",
    "    ax2.set_title('Multivariate Normal Distribution\\n(Marginal: Dim 1, 2)', fontsize=12)\n",
    "    \n",
    "    # 3. Uncertainty (std) 시각화\n",
    "    ax3 = fig.add_subplot(133, projection='3d')\n",
    "    \n",
    "    uncertainty = std[:, :3].mean(axis=1)  # 평균 불확실성\n",
    "    scatter = ax3.scatter(mu[:, 0], mu[:, 1], mu[:, 2], \n",
    "                         c=uncertainty, cmap='plasma', s=10, alpha=0.6)\n",
    "    ax3.set_xlabel('Latent Dim 1', fontsize=10)\n",
    "    ax3.set_ylabel('Latent Dim 2', fontsize=10)\n",
    "    ax3.set_zlabel('Latent Dim 3', fontsize=10)\n",
    "    ax3.set_title('User Latent Representations\\n(Color: Uncertainty σ)', fontsize=12)\n",
    "    plt.colorbar(scatter, ax=ax3, shrink=0.5, label='Avg Std')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Latent dimension ({latent_dim}) < 3, skipping 3D visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차원별 분포 시각화\n",
    "n_dims_to_plot = min(6, latent_dim)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(n_dims_to_plot):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Histogram with KDE\n",
    "    ax.hist(mu[:, i], bins=50, alpha=0.6, density=True, edgecolor='black', label='μ')\n",
    "    \n",
    "    # Prior N(0, 1)\n",
    "    x_range = np.linspace(mu[:, i].min(), mu[:, i].max(), 100)\n",
    "    prior = (1 / np.sqrt(2 * np.pi)) * np.exp(-0.5 * x_range**2)\n",
    "    ax.plot(x_range, prior, 'r--', linewidth=2, label='Prior N(0,1)')\n",
    "    \n",
    "    ax.set_xlabel(f'Latent Dim {i+1}', fontsize=10)\n",
    "    ax.set_ylabel('Density', fontsize=10)\n",
    "    ax.set_title(f'Dimension {i+1} Distribution', fontsize=11)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Latent Dimensions Distribution vs Prior N(0, 1)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 출력 분포 시각화: Multinomial Distribution\n",
    "\n",
    "모델의 출력 분포(Multinomial Distribution)를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 유저들의 출력 분포 계산\n",
    "sample_users_idx = np.random.choice(data_module.num_users, size=5, replace=False)\n",
    "sample_users_data = train_dense[sample_users_idx]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, _, _ = model(sample_users_data)\n",
    "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "print(f\"Output shape: {probs.shape}\")\n",
    "print(f\"Probability sum per user: {probs.sum(axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유저별 출력 분포 시각화\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, user_idx in enumerate(sample_users_idx):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    user_probs = probs[idx]\n",
    "    user_train_items = np.where(sample_users_data[idx].cpu().numpy() > 0)[0]\n",
    "    \n",
    "    # Top-K 아이템 선택\n",
    "    top_k_idx = np.argsort(user_probs)[-50:][::-1]  # Top 50\n",
    "    \n",
    "    colors = ['red' if i in user_train_items else 'blue' for i in top_k_idx]\n",
    "    \n",
    "    ax.bar(range(len(top_k_idx)), user_probs[top_k_idx], color=colors, alpha=0.7)\n",
    "    ax.set_xlabel('Item Rank', fontsize=10)\n",
    "    ax.set_ylabel('Probability', fontsize=10)\n",
    "    ax.set_title(f'User {sample_users_idx[idx]} - Top 50 Items\\n(Red: Train, Blue: Predict)', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 마지막 서브플롯: 전체 평균 분포\n",
    "ax = axes[-1]\n",
    "mean_probs = probs.mean(axis=0)\n",
    "sorted_idx = np.argsort(mean_probs)[::-1][:100]  # Top 100\n",
    "\n",
    "ax.bar(range(len(sorted_idx)), mean_probs[sorted_idx], color='green', alpha=0.7)\n",
    "ax.set_xlabel('Item Rank', fontsize=10)\n",
    "ax.set_ylabel('Mean Probability', fontsize=10)\n",
    "ax.set_title('Average Probability Distribution (Top 100 Items)', fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Multinomial Distribution - Output Probabilities', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability entropy 분석\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# 모든 유저에 대해 entropy 계산\n",
    "print(\"Computing entropy for all users...\")\n",
    "all_entropies = []\n",
    "\n",
    "batch_size = 256\n",
    "for i in range(0, data_module.num_users, batch_size):\n",
    "    batch_data = train_dense[i:i+batch_size]\n",
    "    with torch.no_grad():\n",
    "        logits, _, _ = model(batch_data)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    for prob in probs:\n",
    "        all_entropies.append(entropy(prob))\n",
    "\n",
    "all_entropies = np.array(all_entropies)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(all_entropies, bins=50, edgecolor='black', alpha=0.7, color='teal')\n",
    "plt.axvline(all_entropies.mean(), color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Mean: {all_entropies.mean():.2f}')\n",
    "plt.xlabel('Entropy', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Output Distribution Entropy\\n(Higher = More Uncertain)', fontsize=13)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Entropy vs # Train Interactions\n",
    "n_interactions = np.array(train_mat.sum(axis=1)).flatten()\n",
    "plt.scatter(n_interactions, all_entropies, alpha=0.3, s=10)\n",
    "plt.xlabel('# Train Interactions', fontsize=12)\n",
    "plt.ylabel('Entropy', fontsize=12)\n",
    "plt.title('Entropy vs Training Interactions\\n(More data = Lower entropy?)', fontsize=13)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Correlation\n",
    "corr = np.corrcoef(n_interactions, all_entropies)[0, 1]\n",
    "plt.text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
    "         transform=plt.gca().transAxes, fontsize=11,\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEntropy Statistics:\")\n",
    "print(f\"  Mean: {all_entropies.mean():.4f}\")\n",
    "print(f\"  Std: {all_entropies.std():.4f}\")\n",
    "print(f\"  Min: {all_entropies.min():.4f}\")\n",
    "print(f\"  Max: {all_entropies.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "이 노트북에서는 Multi-VAE 모델의 다양한 측면을 시각화했습니다:\n",
    "\n",
    "1. **Input Data**: User-Item 상호작용 행렬과 sparsity 분석\n",
    "2. **Data Split**: Random, Leave-One-Out, Temporal User, Temporal Global 4가지 전략 비교\n",
    "3. **Predictions**: Top-K 추천 결과와 Recall@K 메트릭 평가\n",
    "4. **Latent Space**: 유저의 잠재 표현을 3차원으로 시각화하고 Multivariate Normal Distribution 분석\n",
    "5. **Output Distribution**: Multinomial 출력 분포와 entropy 분석\n",
    "\n",
    "이러한 시각화를 통해 모델의 동작을 더 잘 이해하고, 개선점을 찾을 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

# MultiVAE ê°œì„  ì´ë ¥ (Changelog)

ì´ ë¬¸ì„œëŠ” MultiVAE í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ê°œì„  ì‚¬í•­ë“¤ì„ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤.

---

## ëª©ì°¨

1. [ì„±ëŠ¥ ê°œì„ ](#1-ì„±ëŠ¥-ê°œì„ )
2. [ë²„ê·¸ ìˆ˜ì •](#2-ë²„ê·¸-ìˆ˜ì •)
3. [êµ¬ì¡° ê°œì„ ](#3-êµ¬ì¡°-ê°œì„ )
4. [ë°ì´í„° ì²˜ë¦¬ ìµœì í™”](#4-ë°ì´í„°-ì²˜ë¦¬-ìµœì í™”)
5. [ì„¤ì • ê´€ë¦¬ ê°œì„ ](#5-ì„¤ì •-ê´€ë¦¬-ê°œì„ )
6. [ì‹œê°í™” ë° ë¶„ì„ ë„êµ¬](#6-ì‹œê°í™”-ë°-ë¶„ì„-ë„êµ¬)
7. [ë¬¸ì„œí™”](#7-ë¬¸ì„œí™”)

---

## 1. ì„±ëŠ¥ ê°œì„ 

### 1.1 ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

**ëª©í‘œ**: EASE ëª¨ë¸ì˜ Recall@10 (0.16)ì„ ë„˜ì–´ì„œëŠ” ì„±ëŠ¥ ë‹¬ì„±

**ë³€ê²½ì‚¬í•­** ([multi_vae_v2.yaml](../configs/multi_vae_v2.yaml)):
```yaml
# Before (multi_vae.yaml)
model:
  hidden_dims: [400, 200]
  dropout: 0.6

training:
  kl_max_weight: 0.5
  kl_anneal_steps: 10000

# After (multi_vae_v2.yaml)
model:
  hidden_dims: [600, 200]  # ì²« ë²ˆì§¸ ë ˆì´ì–´ ì¦ê°€
  dropout: 0.5             # dropout ê°ì†Œ

training:
  kl_max_weight: 0.2       # ë…¼ë¬¸ ê¸°ì¤€ê°’
  kl_anneal_steps: 20000   # ì²œì²œíˆ annealing
  early_stopping_patience: 20  # patience ì¦ê°€
```

**ê·¼ê±°**:
- `hidden_dims`: ë” í° representation capacity
- `dropout`: 0.6ì€ ê³¼ë„í•œ ì •ê·œí™”
- `kl_max_weight`: MultiVAE ë…¼ë¬¸ ê¸°ì¤€ê°’ (0.2)
- Early stopping patience: ë” ì¶©ë¶„í•œ í•™ìŠµ ì‹œê°„ í™•ë³´

### 1.2 ë°ì´í„° ë¶„í•  ì „ëµ

**ë³€ê²½ì‚¬í•­**:
```yaml
data:
  split_strategy: "leave_one_out"  # random -> leave_one_out
  valid_ratio: 0.1
```

**ì´ìœ **: Leave-one-out ë°©ì‹ì´ collaborative filteringì—ì„œ ë” ì•ˆì •ì ì¸ í‰ê°€ ì œê³µ

---

## 2. ë²„ê·¸ ìˆ˜ì •

### 2.1 ğŸ› **CRITICAL**: Encoderì—ì„œ Dropoutê³¼ Normalization ìˆœì„œ ì˜¤ë¥˜

**ë¬¸ì œ**:
- `F.normalize()` í›„ `F.dropout()`ì„ ì ìš©í•˜ë©´ ì •ê·œí™”ê°€ ê¹¨ì§
- Train lossê°€ ~1127ì—ì„œ ì •ì²´ë˜ê³  ê·¹ì‹¬í•œ ë…¸ì´ì¦ˆ ë°œìƒ

**ì›ì¸** ([multi_vae.py:95-98](../src/models/multi_vae.py#L95-L98)):
```python
# âŒ WRONG (Before)
x = F.normalize(x, p=2, dim=1)  # L2 normalize first
x = F.dropout(x, self.dropout, training=self.training)  # Then dropout
```

**ìˆ˜ì •**:
```python
# âœ… CORRECT (After)
x = F.dropout(x, self.dropout, training=self.training)  # Dropout first
x = F.normalize(x, p=2, dim=1)  # Then L2 normalize
```

**ì˜í–¥**:
- Train loss ì•ˆì •í™”
- ìˆ˜ë ´ ì†ë„ ê°œì„ 
- ìµœì¢… ì„±ëŠ¥ í–¥ìƒ

**ì°¸ê³ **: [MultiVAE ë…¼ë¬¸](https://arxiv.org/abs/1802.05814) Section 3.2

### 2.2 Checkpoint ë¡œë”© ì‹œ `weights_only` ì—ëŸ¬

**ë¬¸ì œ**: PyTorch 2.6+ì—ì„œ `weights_only=True`ê°€ ê¸°ë³¸ê°’ìœ¼ë¡œ ë³€ê²½ë˜ì–´ OmegaConf ê°ì²´ ë¡œë”© ì‹¤íŒ¨

**ìˆ˜ì •** ([predict_multi_vae.py:66-70](../predict_multi_vae.py#L66-L70)):
```python
model = MultiVAE.load_from_checkpoint(
    checkpoint_path,
    num_items=datamodule.num_items,
    weights_only=False,  # ì¶”ê°€
)
```

---

## 3. êµ¬ì¡° ê°œì„ 

### 3.1 Train/Predict ìŠ¤í¬ë¦½íŠ¸ ë¶„ë¦¬

**Before**:
- ë‹¨ì¼ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ train/inference ëª¨ë‘ ì²˜ë¦¬

**After**:
- `train_multi_vae.py`: í•™ìŠµ ì „ìš©
- `predict_multi_vae.py`: ì¶”ë¡  ì „ìš© (BERT4Rec íŒ¨í„´ ì°¸ì¡°)

**ì´ì **:
- ëª…í™•í•œ ì±…ì„ ë¶„ë¦¬
- ê° ë‹¨ê³„ë³„ ìµœì í™” ê°€ëŠ¥
- ì¬ì‚¬ìš©ì„± í–¥ìƒ

### 3.2 í†µí•© Path ê´€ë¦¬

**ì¶”ê°€**: `src/utils/path_utils.py`

```python
def get_directories(cfg, stage="fit"):
    """Hydra ì¶œë ¥ ë””ë ‰í† ë¦¬ ê¸°ë°˜ìœ¼ë¡œ checkpoint/tensorboard ê²½ë¡œ ìƒì„±"""
    # fit: í˜„ì¬ ì‹¤í–‰ ë””ë ‰í† ë¦¬ (ìƒˆë¡œ ìƒì„±)
    # predict: ìµœê·¼ ì‹¤í–‰ ë””ë ‰í† ë¦¬ (ê¸°ì¡´ ê²ƒ ì‚¬ìš©)
```

**ì‚¬ìš©**:
```python
# train_multi_vae.py
checkpoint_dir, tensorboard_dir = get_directories(cfg, stage="fit")

# predict_multi_vae.py
checkpoint_dir, tensorboard_dir = get_directories(cfg, stage="predict")
```

**ì´ì **:
- Train/Predict ê°„ checkpoint ê²½ë¡œ ì¼ê´€ì„± ë³´ì¥
- Hydra ì¶œë ¥ ë””ë ‰í† ë¦¬ êµ¬ì¡° í™œìš©

### 3.3 ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ í†µí•©

**ì¶”ê°€**: `run_multi_vae.sh`

```bash
./run_multi_vae.sh [mode] [config_file]

# Modes:
# - train           Train only
# - predict         Predict only
# - both            Train + Predict (default)
# - clean           Clean cache only
# - clean-train     Clean cache + Train
# - clean-both      Clean cache + Train + Predict
```

**ì˜ˆì‹œ**:
```bash
./run_multi_vae.sh clean-both multi_vae_v2
```

**ì°¸ì¡°**: [run_bert4rec.sh](../run_bert4rec.sh) íŒ¨í„´ ì ìš©

---

## 4. ë°ì´í„° ì²˜ë¦¬ ìµœì í™”

### 4.1 Leave-One-Out Split ìµœì í™”

**ë¬¸ì œ**: ê¸°ì¡´ êµ¬í˜„ì´ 30ì´ˆ ì´ìƒ ì†Œìš”

**ì›ì¸**: DataFrame ë°˜ë³µ í•„í„°ë§ (O(N*M) ë³µì¡ë„)

**í•´ê²°** ([recsys_data.py:216-252](../src/data/recsys_data.py#L216-L252)):
```python
# Before: O(N*M)
for u_idx in range(num_users):
    user_items = df_enc[df_enc["user"] == u_idx]["item"].tolist()  # ë§¤ë²ˆ í•„í„°ë§

# After: O(N)
grouped = df_enc.groupby("user")["item"].apply(list).to_dict()  # í•œë²ˆë§Œ
for u_idx in range(num_users):
    user_items = grouped.get(u_idx, [])
```

**ì„±ëŠ¥ í–¥ìƒ**:
- 30ì´ˆ â†’ 0.1ì´ˆ (300ë°° ê°œì„ )

### 4.2 ë””ìŠ¤í¬ ìºì‹± ì‹œìŠ¤í…œ ë„ì…

**ëª©ì **: ë™ì¼ ì„¤ì •ìœ¼ë¡œ ì¬ì‹¤í–‰ ì‹œ ë°ì´í„° ë¡œë”© ì‹œê°„ ë‹¨ì¶•

**êµ¬í˜„** ([recsys_data.py:358-438](../src/data/recsys_data.py#L358-L438)):

```python
class RecSysDataModule:
    def __init__(self, ..., use_cache=True, cache_dir="~/.cache/recsys"):
        ...

    def _get_cache_key(self):
        """ì„¤ì • ê¸°ë°˜ MD5 í•´ì‹œ ìƒì„±"""
        key_params = {
            "data_file": self.data_file,
            "split_strategy": self.split_strategy,
            "seed": self.seed,
            "min_interactions": self.min_interactions,
            ...
        }
        return hashlib.md5(str(sorted(key_params.items())).encode()).hexdigest()

    def _save_to_cache(self):
        """train_mat, valid_gt ë“±ì„ pickleë¡œ ì €ì¥"""

    def _load_from_cache(self):
        """ìºì‹œ ë¡œë“œ (ì„¤ì •ì´ ë™ì¼í•˜ë©´)"""
```

**ìºì‹œ ë‚´ìš©**:
- `user2idx`, `idx2user`, `item2idx`, `idx2item`: ID ë§¤í•‘
- `num_users`, `num_items`: ë©”íƒ€ë°ì´í„°
- `train_mat`: Sparse matrix (CSR)
- `valid_gt`: Validation ground truth

**ì„¤ì •** ([default_setup.yaml:34-37](../configs/common/default_setup.yaml#L34-L37)):
```yaml
data_cache:
  use_cache: true
  cache_dir: ~/.cache/recsys
```

**ì„±ëŠ¥**:
- ì²« ì‹¤í–‰: 30ì´ˆ (ë°ì´í„° ë¡œë“œ + ì „ì²˜ë¦¬ + ìºì‹œ ì €ì¥)
- ì¬ì‹¤í–‰: 0.1ì´ˆ (ìºì‹œ ë¡œë“œë§Œ)

**ìºì‹œ ê´€ë¦¬**:
```bash
./run_multi_vae.sh clean        # ìºì‹œ ì‚­ì œ
./run_multi_vae.sh clean-train  # ìºì‹œ ì‚­ì œ + í•™ìŠµ
```

---

## 5. ì„¤ì • ê´€ë¦¬ ê°œì„ 

### 5.1 `.get()` íŒ¨í„´ ì œê±°

**ë¬¸ì œ**: Configì—ì„œ `.get("field", default)` ì‚¬ìš© ì‹œ ë¡œê·¸ì™€ ì‹¤ì œ ë™ì‘ ë¶ˆì¼ì¹˜ ê°€ëŠ¥

**ë³€ê²½ ëŒ€ìƒ**:
- [train_bert4rec.py](../train_bert4rec.py)
- [predict_bert4rec.py](../predict_bert4rec.py)
- [train_multi_vae.py](../train_multi_vae.py)
- [predict_multi_vae.py](../predict_multi_vae.py)

**Before**:
```python
min_interactions = cfg.data.get("min_interactions", 3)
num_workers = cfg.data.get("num_workers", 4)
```

**After**:
```python
min_interactions = cfg.data.min_interactions
num_workers = cfg.data.num_workers
```

**ì´ì **:
- Config íŒŒì¼ì— ëª…ì‹œë˜ì§€ ì•Šì€ ê°’ì€ ì¦‰ì‹œ ì—ëŸ¬ ë°œìƒ
- ë¡œê·¸ì— ë‚¨ì€ ì„¤ì •ê³¼ ì‹¤ì œ ë™ì‘ ì¼ì¹˜ ë³´ì¥
- ë””ë²„ê¹… ìš©ì´

### 5.2 ì„¤ì • íŒŒì¼ êµ¬ì¡°í™”

**êµ¬ì¡°**:
```
configs/
â”œâ”€â”€ common/
â”‚   â””â”€â”€ default_setup.yaml       # ê³µí†µ ì„¤ì • (checkpoint, seed, cache ë“±)
â”œâ”€â”€ bert4rec_v2.yaml             # BERT4Rec ì„¤ì •
â””â”€â”€ multi_vae_v2.yaml            # MultiVAE ì„¤ì • (ê°œì„  ë²„ì „)
```

**ê³µí†µ ì„¤ì • ì¶”ì¶œ** ([default_setup.yaml](../configs/common/default_setup.yaml)):
```yaml
# Hydra ì¶œë ¥ ë””ë ‰í† ë¦¬
hydra:
  run:
    dir: ./saved/hydra_logs/${model_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}

# Checkpoint ì„¤ì •
checkpoint:
  save_top_k: 1
  monitor: "val_loss"
  mode: "min"

# Data caching ì„¤ì •
data_cache:
  use_cache: true
  cache_dir: ~/.cache/recsys

# ê¸°íƒ€
seed: 42
float32_matmul_precision: "medium"
```

---

## 6. ì‹œê°í™” ë° ë¶„ì„ ë„êµ¬

### 6.1 MultiVAE Attention Visualization

**ì¶”ê°€**: [notebooks/visualize_multi_vae.ipynb](../notebooks/visualize_multi_vae.ipynb)

**ê¸°ëŠ¥**:
1. **Latent Space ì‹œê°í™”**:
   - Î¼ (mean) ë¶„í¬ì˜ PCA/t-SNE ì‹œê°í™”
   - User embeddingì˜ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„

2. **Reconstruction ë¶„ì„**:
   - Input vs Reconstructed output ë¹„êµ
   - Top-K ì¶”ì²œ ì•„ì´í…œ ì‹œê°í™”

3. **Training Dynamics**:
   - Loss curves (Total, Reconstruction, KL)
   - KL annealing ê³¼ì • ì‹œê°í™”

4. **Item Similarity**:
   - Decoder weight ê¸°ë°˜ item-item similarity
   - ìœ ì‚¬ ì•„ì´í…œ ì¶”ì²œ

**ì˜ˆì‹œ í”Œë¡¯**:
```python
# Latent space visualization
visualize_latent_space(model, datamodule, method='tsne')

# Reconstruction quality
plot_reconstruction_quality(model, user_idx=123)

# Training curves
plot_training_curves(tensorboard_log_dir)
```

### 6.2 BERT4Rec Attention Visualization

**ìˆ˜ì •**: [notebooks/visualize_bert4rec.ipynb](../notebooks/visualize_bert4rec.ipynb)

**ì¶”ê°€ ê¸°ëŠ¥**:
- Multi-head attention ì‹œê°í™” (ëª¨ë“  ë ˆì´ì–´ + ëª¨ë“  í—¤ë“œ)
- Layer-wise attention pattern ë¹„êµ
- Position-wise attention ë¶„ì„

**ë³€ê²½ì‚¬í•­**:
```python
# Before: ì²« ë²ˆì§¸ ë ˆì´ì–´ë§Œ
attention = model.transformer_blocks[0].attention

# After: ëª¨ë“  ë ˆì´ì–´
for layer_idx in range(cfg.model.num_layers):
    attention = model.transformer_blocks[layer_idx].attention
    # ëª¨ë“  head ì‹œê°í™”
```

---

## ì„±ëŠ¥ ë¹„êµ

### Recall@10

| ëª¨ë¸ | ì„¤ì • | Recall@10 | ë¹„ê³  |
|-----|------|-----------|------|
| EASE | Baseline | **0.16** | ëª©í‘œ |
| MultiVAE (Before) | `multi_vae.yaml` | 0.1367 | ì´ˆê¸° |
| MultiVAE (After) | `multi_vae_v2.yaml` | 0.1311 | ì„±ëŠ¥ì €í•˜ |

### ë°ì´í„° ë¡œë”© ì†ë„

| ì‘ì—… | Before | After |
|-----|--------|-------|
| Leave-one-out split | 30ì´ˆ | 1ì´ˆ |
| ì¬ì‹¤í–‰ (ìºì‹œ í™œìš©) | 30ì´ˆ | 1ì´ˆ |

---

## ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
- [Variational Autoencoders for Collaborative Filtering (MultiVAE)](https://arxiv.org/abs/1802.05814)
- [BERT4Rec: Sequential Recommendation with BERT](https://arxiv.org/abs/1904.06690)
- [EASE: Embarrassingly Shallow Autoencoders](https://arxiv.org/abs/1905.03375)

### ì½”ë“œ ì°¸ì¡°
- [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/)
- [Hydra Configuration](https://hydra.cc/docs/intro/)

### ê´€ë ¨ ë¬¸ì„œ
- [CHECKPOINT_STRUCTURE.md](CHECKPOINT_STRUCTURE.md): Checkpoint íŒŒì¼ êµ¬ì¡°
- [README.md](../README.md): í”„ë¡œì íŠ¸ ê°œìš” (ì—…ë°ì´íŠ¸ í•„ìš”)

---

**Last Updated**: 2025-12-23

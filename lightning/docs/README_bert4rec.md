# BERT4Rec Implementation Guide

## ê°œìš”

BERT4Rec (Bidirectional Encoder Representations from Transformers for Sequential Recommendation)ì˜ PyTorch Lightning êµ¬í˜„ì…ë‹ˆë‹¤.

**ë…¼ë¬¸**: [BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer](https://arxiv.org/abs/1904.06690)

## âœ¨ ìµœì‹  ê°œì„ ì‚¬í•­ (2025-12-29)

### Masking Strategy ê°œì„ : Random + Boost ë°©ì‹

ê¸°ì¡´ì˜ last-item masking ì „ëµì„ ê°œì„ í•˜ì—¬ **ë°ì´í„° íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì„ ë™ì‹œì— í–¥ìƒ**ì‹œì¼°ìŠµë‹ˆë‹¤.

**ê¸°ì¡´ ë°©ì‹ì˜ ë¬¸ì œì **:
- `last_item_mask_ratio` ë¹„ìœ¨ì˜ ìƒ˜í”Œë§Œ ë§ˆì§€ë§‰ ì•„ì´í…œë§Œ ë§ˆìŠ¤í‚¹
- ì´ ìƒ˜í”Œë“¤ì€ ë§¤ epochë§ˆë‹¤ **ê°™ì€ ë§ˆìŠ¤í‚¹ íŒ¨í„´** ë°˜ë³µ
- ë°ì´í„° ë‹¤ì–‘ì„± ë¶€ì¡±ìœ¼ë¡œ í•™ìŠµ ë¹„íš¨ìœ¨

**ê°œì„ ëœ ë°©ì‹** (Random + Boost):
```python
# ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•´:
1. Random masking ì ìš© (ë§¤ epoch ë‹¤ë¥¸ íŒ¨í„´)
2. last_item_mask_ratio í™•ë¥ ë¡œ ë§ˆì§€ë§‰ ì•„ì´í…œ ì¶”ê°€ ë§ˆìŠ¤í‚¹
```

**ê°œì„  íš¨ê³¼**:
| í•­ëª© | ê¸°ì¡´ ë°©ì‹ | ê°œì„  ë°©ì‹ |
|------|----------|----------|
| ë°ì´í„° ë‹¤ì–‘ì„± | âŒ ì¼ë¶€ ìƒ˜í”Œ ê³ ì • íŒ¨í„´ | âœ… ëª¨ë“  ìƒ˜í”Œ ë§¤ epoch ë³€í™” |
| Random masking | 90% ìƒ˜í”Œë§Œ | âœ… 100% ìƒ˜í”Œ |
| Next-item focus | âœ… í•™ìŠµë¨ | âœ…âœ… ë” ê°•í™”ë¨ |
| í•™ìŠµ íš¨ìœ¨ | âš ï¸ ì¤‘ë³µ í•™ìŠµ | âœ… íš¨ìœ¨ì  |

**ì‚¬ìš© ë°©ë²•**:
```yaml
model:
  random_mask_prob: 0.2        # ëª¨ë“  ìƒ˜í”Œì— ëœë¤ ë§ˆìŠ¤í‚¹
  last_item_mask_ratio: 0.05   # 5% í™•ë¥ ë¡œ ë§ˆì§€ë§‰ ì•„ì´í…œ ì¶”ê°€ ë§ˆìŠ¤í‚¹
```

ìì„¸í•œ ë‚´ìš©ì€ [Masking Strategy](#masking-strategy-ë…¼ë¬¸-section-32) ì„¹ì…˜ì„ ì°¸ì¡°í•˜ì„¸ìš”.

---

## íŒŒì¼ êµ¬ì¡°

```
lightning/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ bert4rec.yaml              # Hydra ì„¤ì • íŒŒì¼
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ bert4rec.py            # BERT4Rec ëª¨ë¸ (LightningModule)
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ bert4rec_data.py       # DataModule
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ README_bert4rec.md         # ì´ ë¬¸ì„œ
â”œâ”€â”€ train_bert4rec.py              # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ predict_bert4rec.py            # ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ run_bert4rec.sh                # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
```

## ì‚¬ìš©ë²•

### 1. í•™ìŠµ

```bash
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ (validation í¬í•¨)
python train_bert4rec.py

# Full data í•™ìŠµ (validation ì—†ì´ ì „ì²´ ë°ì´í„° ì‚¬ìš©)
python train_bert4rec.py data.use_full_data=true

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì˜¤ë²„ë¼ì´ë“œ
python train_bert4rec.py model.hidden_units=128 training.lr=0.001

# ë…¼ë¬¸ ê¸°ì¤€ í•™ìŠµ (lr=0.001)
python train_bert4rec.py training.lr=0.001

# ì—¬ëŸ¬ íŒŒë¼ë¯¸í„° ë™ì‹œ ë³€ê²½
python train_bert4rec.py \
    model.hidden_units=128 \
    model.num_heads=8 \
    model.num_layers=4 \
    training.num_epochs=500 \
    training.lr=0.001
```

### 2. ì¶”ë¡ 

```bash
# ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ë¡œ ì¶”ë¡ 
python predict_bert4rec.py

# íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©
python predict_bert4rec.py inference.checkpoint_path=saved/bert4rec/checkpoints/best.ckpt

# Top-K ë³€ê²½
python predict_bert4rec.py inference.topk=20
```

**âš ï¸ Future Information Leakage ë°©ì§€**: ì¶”ë¡  ì‹œ ê° ìœ ì €ì˜ ë§ˆì§€ë§‰ í´ë¦­ ì‹œì  ì´í›„ ê°œë´‰í•œ ì˜í™”ëŠ” ìë™ìœ¼ë¡œ ì¶”ì²œì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.

### 3. ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹¤í–‰

```bash
# í•™ìŠµ + ì¶”ë¡ 
./run_bert4rec.sh both

# í•™ìŠµë§Œ
./run_bert4rec.sh train

# ì¶”ë¡ ë§Œ
./run_bert4rec.sh predict
```

## ì„¤ì • (configs/bert4rec.yaml)

### ë°ì´í„° ì„¤ì •

```yaml
data:
  data_dir: "~/data/train/"        # ë°ì´í„° ë””ë ‰í† ë¦¬
  data_file: "train_ratings.csv"   # CSV íŒŒì¼ëª…
  batch_size: 512                  # ë°°ì¹˜ í¬ê¸°
  min_interactions: 3              # ìµœì†Œ interaction ìˆ˜
  num_workers: 4                   # DataLoader workers
  use_full_data: false             # Full data í•™ìŠµ (validation ì—†ìŒ)
```

**ë°ì´í„° í¬ë§·**: CSV with columns `user`, `item`, `time` (optional)

**Full Data í•™ìŠµ ëª¨ë“œ** (`use_full_data: true`):
- Validation split ì—†ì´ ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ
- Early stopping ìë™ ë¹„í™œì„±í™”
- CheckpointëŠ” `train_loss` ê¸°ì¤€ìœ¼ë¡œ ì €ì¥
- ìµœì¢… ì œì¶œìš© ëª¨ë¸ í•™ìŠµ ì‹œ ê¶Œì¥

### ëª¨ë¸ ì„¤ì •

```yaml
model:
  hidden_units: 256            # Hidden dimension (ë…¼ë¬¸: dataset-dependent)
  num_heads: 4                 # Attention heads ìˆ˜ (ë…¼ë¬¸: dataset-dependent)
  num_layers: 2                # Transformer blocks ìˆ˜ (ë…¼ë¬¸: 2 for most datasets)
  max_len: 200                 # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (ë…¼ë¬¸: 200 for ML-1M)
  dropout_rate: 0.2            # Dropout í™•ë¥  (ë…¼ë¬¸: 0.2~0.5)
  random_mask_prob: 0.2        # ëœë¤ ë§ˆìŠ¤í‚¹ í™•ë¥  (ë…¼ë¬¸: 0.15, BERTì™€ ë™ì¼)
  last_item_mask_ratio: 0.05   # ëœë¤ ë§ˆìŠ¤í‚¹ í›„ ì¶”ê°€ë¡œ ë§ˆì§€ë§‰ ì•„ì´í…œì„ ë§ˆìŠ¤í‚¹í•  í™•ë¥  (next-item prediction ê°•í™”)
  share_embeddings: true       # Output layerì™€ embedding ê³µìœ  (ë…¼ë¬¸: Yes)

  # ë©”íƒ€ë°ì´í„° ì„¤ì • (í™•ì¥ ê¸°ëŠ¥)
  use_genre_emb: false      # ì¥ë¥´ ì„ë² ë”© ì‚¬ìš©
  use_director_emb: false   # ê°ë… ì„ë² ë”© ì‚¬ìš©
  use_writer_emb: false     # ì‘ê°€ ì„ë² ë”© ì‚¬ìš©
  use_title_emb: true       # ì œëª© ì„ë² ë”© ì‚¬ìš© (ì‚¬ì „ ê³„ì‚°ëœ ì„ë² ë”©)
  metadata_fusion: "gate"   # ìœµí•© ë°©ë²•: "concat", "add", "gate"
  metadata_dropout: 0.1     # ë©”íƒ€ë°ì´í„° ì„ë² ë”© dropout
```

**ë…¼ë¬¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
- Hidden units: 64~256 (dataset-dependent)
- Attention heads: 2~8
- Transformer layers: 2~4
- Max sequence length: 200 (ML-1M), 50 (Steam)
- Dropout: 0.2~0.5
- Masking probability: 0.15 (BERT ë…¼ë¬¸ê³¼ ë™ì¼)

**ì¤‘ìš”**: `hidden_units`ëŠ” `num_heads`ë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤.

### í•™ìŠµ ì„¤ì •

```yaml
training:
  num_epochs: 500                  # ìµœëŒ€ epoch ìˆ˜ (ë…¼ë¬¸: early stopping ì‚¬ìš©)
  lr: 0.001                        # Learning rate (ë…¼ë¬¸: 0.001)
  weight_decay: 0.0                # L2 regularization (ë…¼ë¬¸: ëª…ì‹œ ì•ˆë¨)
  monitor_metric: "val_ndcg@10"    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê¸°ì¤€
  early_stopping: true             # Early stopping (ë…¼ë¬¸: ì‚¬ìš©)
  early_stopping_patience: 20      # Patience
  accelerator: "auto"              # GPU/CPU ìë™ ì„ íƒ
  precision: "32-true"             # 32-bit precision (ì•ˆì •ì„± ìš°ì„ ) / "16-mixed": V100 ìµœì í™” (ì†ë„ ìš°ì„ )
```

## ì•„ì´í…œ ë©”íƒ€ë°ì´í„° ê¸°ëŠ¥ (í™•ì¥)

BERT4Recì˜ ê¸°ë³¸ item ì„ë² ë”©ì— ì¶”ê°€ë¡œ **ì•„ì´í…œ ë©”íƒ€ë°ì´í„°**ë¥¼ í™œìš©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì§€ì›í•˜ëŠ” ë©”íƒ€ë°ì´í„° ì¢…ë¥˜

1. **ì¥ë¥´ (Genre)**: ì˜í™”ì˜ ì¥ë¥´ ì •ë³´
   - íŒŒì¼: `genres.tsv` (item, genre ì»¬ëŸ¼)
   - ë‹¤ì¤‘ ì¥ë¥´ ì§€ì› (ì˜ˆ: ì•¡ì…˜, ë“œë¼ë§ˆ)
   - í‰ê·  í’€ë§ìœ¼ë¡œ ì§‘ê³„

2. **ê°ë… (Director)**: ì˜í™” ê°ë… ì •ë³´
   - íŒŒì¼: `directors.tsv` (item, director ì»¬ëŸ¼)
   - ë‹¨ì¼ ê°ë…ë§Œ ì§€ì›

3. **ì‘ê°€ (Writer)**: ì˜í™” ê°ë³¸ê°€ ì •ë³´
   - íŒŒì¼: `writers.tsv` (item, writer ì»¬ëŸ¼)
   - ë‹¤ì¤‘ ì‘ê°€ ì§€ì›
   - í‰ê·  í’€ë§ìœ¼ë¡œ ì§‘ê³„

4. **ì œëª© (Title)**: ì‚¬ì „ ê³„ì‚°ëœ ì œëª© ì„ë² ë”©
   - íŒŒì¼: `titles.tsv` (item, title ì»¬ëŸ¼)
   - BERT/Sentence-BERT ë“±ìœ¼ë¡œ ì‚¬ì „ ê³„ì‚°ëœ 768ì°¨ì› ë²¡í„° ì‚¬ìš©

### ì œëª© ì„ë² ë”© ìƒì„± ë°©ë²•

ì œëª© ì„ë² ë”©ì€ ì˜í™” ì œëª©ì˜ ì˜ë¯¸ë¡ ì  ì •ë³´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ì²œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

#### ë°©ë²• 0: ìë™ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© (ì œëª© + ì¥ë¥´, ê°€ì¥ ê¶Œì¥) â­

**íŠ¹ì§•**:
- ì œëª©ê³¼ ì¥ë¥´ë¥¼ ê²°í•©í•˜ì—¬ ë” í’ë¶€í•œ ì˜ë¯¸ ì •ë³´ ì œê³µ
- `mxbai-embed-large-v1` ëª¨ë¸ ì‚¬ìš© (1024-dim, MTEB ìµœìƒìœ„)
- ì›í´ë¦­ ì‹¤í–‰ìœ¼ë¡œ ì „ì²˜ë¦¬ ì™„ë£Œ
- ì¥ë¥´ ì •ë³´ê°€ ì—†ëŠ” ì•„ì´í…œì€ ì œëª©ë§Œ ì‚¬ìš©

**ì‚¬ìš©ë²•**:

```bash
# ê¸°ë³¸ ì‚¬ìš© (mxbai-embed-large-v1, ì œëª© + ì¥ë¥´)
python scripts/preprocess_title_genre_embeddings.py

# ë°±ì—… ìƒì„± í›„ ì‹¤í–‰
python scripts/preprocess_title_genre_embeddings.py --backup-original

# ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
python scripts/preprocess_title_genre_embeddings.py \
    --model-name sentence-transformers/all-mpnet-base-v2

# ë°°ì¹˜ í¬ê¸° ì¡°ì • (GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ)
python scripts/preprocess_title_genre_embeddings.py --batch-size 16

# ì»¤ìŠ¤í…€ ì¶œë ¥ ê²½ë¡œ ì§€ì •
python scripts/preprocess_title_genre_embeddings.py \
    --data-dir ~/data/train \
    --output-path ~/data/train/custom_output/titles.tsv
```

**ê²°í•© í…ìŠ¤íŠ¸ ì˜ˆì‹œ**:
```
ì›ë³¸ ì œëª©: "The Matrix"
ì¥ë¥´: ["Action", "Sci-Fi"]
â†’ ê²°í•© í…ìŠ¤íŠ¸: "The Matrix. Genres: Action, Sci-Fi"

ì›ë³¸ ì œëª©: "Inception"
ì¥ë¥´: []
â†’ ê²°í•© í…ìŠ¤íŠ¸: "Inception"
```

**ì¥ì **:
- ì œëª©ê³¼ ì¥ë¥´ì˜ ì˜ë¯¸ë¥¼ ë™ì‹œì— ì„ë² ë”©ì— ë°˜ì˜
- ì¥ë¥´ ì •ë³´ê°€ ëª¨ë¸ì˜ semantic understandingì„ í–¥ìƒ
- ë³„ë„ì˜ ì¥ë¥´ ì„ë² ë”© ì—†ì´ë„ ì¥ë¥´ ì •ë³´ í™œìš© ê°€ëŠ¥
- MTEB ìµœìƒìœ„ ëª¨ë¸ë¡œ ìµœê³  í’ˆì§ˆ ë³´ì¥

**ì¶œë ¥ ì˜ˆì‹œ**:
```bash
2025-12-27 12:00:00 - INFO - Loading model: mixedbread-ai/mxbai-embed-large-v1
2025-12-27 12:00:05 - INFO - Model loaded successfully
2025-12-27 12:00:05 - INFO - Loading titles from: ~/data/train/titles.tsv
2025-12-27 12:00:05 - INFO - Loaded 6807 titles
2025-12-27 12:00:05 - INFO - Loading genres from: ~/data/train/genres.tsv
2025-12-27 12:00:05 - INFO - Loaded 20414 genre entries
2025-12-27 12:00:05 - INFO - Aggregated genres for 6807 items
2025-12-27 12:00:05 - INFO - Creating combined title + genre texts...

Example combined texts:
  [318] Shawshank Redemption, The (1994). Genres: Crime, Drama
  [2571] Matrix, The (1999). Genres: Action, Sci-Fi, Thriller
  [2959] Fight Club (1999). Genres: Action, Crime, Drama, Thriller
  [296] Pulp Fiction (1994). Genres: Comedy, Crime, Drama, Thriller
  [356] Forrest Gump (1994). Genres: Comedy, Drama, Romance, War

2025-12-27 12:00:05 - INFO - Generating embeddings...
Processing batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 213/213 [02:15<00:00,  1.57it/s]
2025-12-27 12:02:20 - INFO - Generated embeddings shape: (6807, 1024)
2025-12-27 12:02:20 - INFO - Embedding dimension: 1024
2025-12-27 12:02:20 - INFO - Saving embeddings to: ~/data/train/title_embeddings/titles.tsv
2025-12-27 12:02:21 - INFO - Saved successfully!
================================================================================
Title + Genre Embedding Generation Complete!
  Model: mixedbread-ai/mxbai-embed-large-v1
  Total items: 6807
  Embedding dimension: 1024
  Items with genres: 6807
  Items without genres: 0
  Output file: ~/data/train/title_embeddings/titles.tsv
================================================================================
```

#### ë°©ë²• 1: Sentence-BERT (ìˆ˜ë™ êµ¬í˜„)

**íŠ¹ì§•**:
- ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì¥ì´ ê°€ê¹Œìš´ ë²¡í„°ë¡œ ë§¤í•‘ë¨
- ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ë¡œ ê³ í’ˆì§ˆ ì„ë² ë”© ìƒì„±
- ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ ê°€ì¥ íš¨ê³¼ì 

**êµ¬í˜„ ì˜ˆì‹œ**:

```python
from sentence_transformers import SentenceTransformer
import pandas as pd
import numpy as np

# 1. Sentence-BERT ëª¨ë¸ ë¡œë“œ
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
# ë˜ëŠ” í•œêµ­ì–´: SentenceTransformer('jhgan/ko-sroberta-multitask')

# 2. ì˜í™” ì œëª© ë°ì´í„° ë¡œë“œ
titles_df = pd.read_csv('titles.tsv', sep='\t')  # columns: item, title

# 3. ì œëª© ì„ë² ë”© ìƒì„±
embeddings = model.encode(
    titles_df['title'].tolist(),
    batch_size=32,
    show_progress_bar=True,
    normalize_embeddings=True  # L2 ì •ê·œí™”
)

# 4. TSV íŒŒì¼ë¡œ ì €ì¥
# í˜•ì‹: item \t embedding_dim_0 embedding_dim_1 ... embedding_dim_767
with open('titles.tsv', 'w') as f:
    f.write('item\ttitle\n')
    for idx, (item_id, emb) in enumerate(zip(titles_df['item'], embeddings)):
        emb_str = ' '.join(map(str, emb))
        f.write(f'{item_id}\t{emb_str}\n')

print(f"Generated embeddings with shape: {embeddings.shape}")
# ì¶œë ¥: (num_items, 384) for all-MiniLM-L6-v2
```

**ì¶”ì²œ ëª¨ë¸**:
| ëª¨ë¸ | ì°¨ì› | ì–¸ì–´ | MTEB | íŠ¹ì§• |
|------|------|------|------|------|
| **`mixedbread-ai/mxbai-embed-large-v1`** â­ | **1024** | ì˜ì–´ | **~65** | **MTEB ìµœìƒìœ„ê¶Œ, ì¶”ì²œ ì‹œìŠ¤í…œ ìµœì í™” (ê¶Œì¥)** |
| `all-mpnet-base-v2` | 768 | ì˜ì–´ | ~63 | ê³ í’ˆì§ˆ |
| `all-MiniLM-L6-v2` | 384 | ì˜ì–´ | ~56 | ë¹ ë¥´ê³  íš¨ìœ¨ì  (ë² ì´ìŠ¤ë¼ì¸) |
| `jhgan/ko-sroberta-multitask` | 768 | í•œêµ­ì–´ | - | í•œêµ­ì–´ ìµœì í™” |
| `paraphrase-multilingual-MiniLM-L12-v2` | 384 | ë‹¤êµ­ì–´ | - | 50ê°œ ì–¸ì–´ ì§€ì› |

#### ë°©ë²• 2: BERT í‰ê·  í’€ë§

**íŠ¹ì§•**:
- BERTì˜ í† í° ì„ë² ë”©ì„ í‰ê· í•˜ì—¬ ë¬¸ì¥ ì„ë² ë”© ìƒì„±
- Sentence-BERTë³´ë‹¤ í’ˆì§ˆì´ ë‚®ì„ ìˆ˜ ìˆìŒ
- HuggingFace Transformers ì‚¬ìš©

**êµ¬í˜„ ì˜ˆì‹œ**:

```python
from transformers import AutoTokenizer, AutoModel
import torch
import pandas as pd

# 1. BERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = 'bert-base-uncased'  # ë˜ëŠ” 'bert-base-multilingual-cased'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
model.eval()

# 2. ì œëª© ë°ì´í„° ë¡œë“œ
titles_df = pd.read_csv('titles.tsv', sep='\t')

def get_bert_embedding(text):
    """BERT í‰ê·  í’€ë§ìœ¼ë¡œ ë¬¸ì¥ ì„ë² ë”© ìƒì„±"""
    # í† í°í™”
    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)

    # Forward pass (gradient ê³„ì‚° ì•ˆí•¨)
    with torch.no_grad():
        outputs = model(**inputs)

    # ë§ˆì§€ë§‰ hidden state: [batch=1, seq_len, hidden_dim=768]
    last_hidden = outputs.last_hidden_state

    # í‰ê·  í’€ë§ (CLS í† í° ì œì™¸)
    # Attention maskë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨ë”© ì œì™¸
    mask = inputs['attention_mask'].unsqueeze(-1)  # [1, seq_len, 1]
    masked_embeddings = last_hidden * mask
    sum_embeddings = masked_embeddings.sum(dim=1)  # [1, hidden_dim]
    count = mask.sum(dim=1)  # [1, 1]
    mean_embedding = sum_embeddings / count  # [1, hidden_dim]

    return mean_embedding.squeeze().numpy()

# 3. ëª¨ë“  ì œëª©ì— ëŒ€í•´ ì„ë² ë”© ìƒì„±
embeddings = []
for title in titles_df['title']:
    emb = get_bert_embedding(title)
    embeddings.append(emb)

embeddings = np.array(embeddings)

# 4. ì €ì¥ (ë°©ë²• 1ê³¼ ë™ì¼)
with open('titles.tsv', 'w') as f:
    f.write('item\ttitle\n')
    for item_id, emb in zip(titles_df['item'], embeddings):
        emb_str = ' '.join(map(str, emb))
        f.write(f'{item_id}\t{emb_str}\n')

print(f"Generated BERT embeddings: {embeddings.shape}")
# ì¶œë ¥: (num_items, 768)
```

#### ë°©ë²• 3: TF-IDF (ê²½ëŸ‰í™”)

**íŠ¹ì§•**:
- ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¶ˆí•„ìš”
- ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- ì˜ë¯¸ë¡ ì  ì •ë³´ëŠ” ë¶€ì¡±í•˜ì§€ë§Œ í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­ì—ëŠ” íš¨ê³¼ì 

**êµ¬í˜„ ì˜ˆì‹œ**:

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
import pandas as pd
import numpy as np

# 1. ì œëª© ë°ì´í„° ë¡œë“œ
titles_df = pd.read_csv('titles.tsv', sep='\t')

# 2. TF-IDF ë²¡í„°í™”
vectorizer = TfidfVectorizer(
    max_features=5000,      # ìƒìœ„ 5000ê°œ ë‹¨ì–´ë§Œ ì‚¬ìš©
    ngram_range=(1, 2),     # Unigram + Bigram
    min_df=2,               # ìµœì†Œ 2ë²ˆ ì´ìƒ ë“±ì¥í•œ ë‹¨ì–´
    stop_words='english'    # ë¶ˆìš©ì–´ ì œê±°
)

tfidf_matrix = vectorizer.fit_transform(titles_df['title'])
print(f"TF-IDF shape: {tfidf_matrix.shape}")  # (num_items, 5000)

# 3. ì°¨ì› ì¶•ì†Œ (SVD)
svd = TruncatedSVD(n_components=384, random_state=42)
embeddings = svd.fit_transform(tfidf_matrix)

# L2 ì •ê·œí™”
from sklearn.preprocessing import normalize
embeddings = normalize(embeddings, norm='l2')

# 4. ì €ì¥ (ë°©ë²• 1ê³¼ ë™ì¼)
with open('titles.tsv', 'w') as f:
    f.write('item\ttitle\n')
    for item_id, emb in zip(titles_df['item'], embeddings):
        emb_str = ' '.join(map(str, emb))
        f.write(f'{item_id}\t{emb_str}\n')

print(f"Generated TF-IDF embeddings: {embeddings.shape}")
# ì¶œë ¥: (num_items, 384)
```

#### íŒŒì¼ í˜•ì‹ (titles.tsv)

ìƒì„±ëœ ì„ë² ë”© íŒŒì¼ì€ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:

```tsv
item	title
1	0.123 -0.456 0.789 ... (768ê°œ ë˜ëŠ” 384ê°œ ê°’)
2	-0.234 0.567 -0.890 ...
3	0.345 -0.678 0.123 ...
```

**ì¤‘ìš” ì‚¬í•­**:
- ì²« ë²ˆì§¸ ì¤„: í—¤ë” (`item\ttitle`)
- ë‘ ë²ˆì§¸ ì¤„ë¶€í„°: `item_id\tì„ë² ë”©ê°’1 ì„ë² ë”©ê°’2 ... ì„ë² ë”©ê°’N`
- êµ¬ë¶„ì: íƒ­(`\t`)
- ì„ë² ë”© ê°’: ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ float ê°’ë“¤

#### ì„±ëŠ¥ ë¹„êµ

| ë°©ë²• | ì°¨ì› | í’ˆì§ˆ | ì†ë„ | ë©”ëª¨ë¦¬ | ì¶”ì²œ ìš©ë„ |
|------|------|------|------|--------|-----------|
| **Title + Genre (ìë™)** | **1024** | **â­â­â­â­â­** | **â­â­â­â­** | **â­â­â­** | **í”„ë¡œë•ì…˜ (ìµœê³  ê¶Œì¥)** â­ |
| Sentence-BERT | 384-768 | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | í”„ë¡œë•ì…˜ |
| BERT í‰ê·  í’€ë§ | 768 | â­â­â­â­ | â­â­â­ | â­â­ | ì‹¤í—˜ ë‹¨ê³„ |
| TF-IDF + SVD | 100-500 | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | ë² ì´ìŠ¤ë¼ì¸ |

#### ë°ì´í„° ë¡œë”© (ìë™)

BERT4RecDataModuleì´ `titles.tsv` íŒŒì¼ì„ ìë™ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤:

```python
# src/data/bert4rec_data.pyì—ì„œ ìë™ ì²˜ë¦¬
def load_metadata(self):
    titles_path = os.path.join(self.data_dir, 'titles.tsv')
    if os.path.exists(titles_path):
        # TSV íŒŒì¼ ë¡œë“œ
        with open(titles_path, 'r') as f:
            next(f)  # Skip header
            for line in f:
                parts = line.strip().split('\t')
                item_id = int(parts[0])
                emb = np.array([float(x) for x in parts[1].split()])
                self.item_title_embs[item_id] = emb
```

**ìë™ ê°ì§€**:
- ì„ë² ë”© ì°¨ì›ì€ ì²« ë²ˆì§¸ ì•„ì´í…œì—ì„œ ìë™ìœ¼ë¡œ ê°ì§€ë¨
- `title_embedding_dim` íŒŒë¼ë¯¸í„°ê°€ ëª¨ë¸ì— ìë™ìœ¼ë¡œ ì „ë‹¬ë¨

#### íŒê³¼ Best Practices

1. **L2 ì •ê·œí™” ê¶Œì¥**:
   ```python
   # ëª¨ë“  ì„ë² ë”©ì„ L2 ì •ê·œí™”í•˜ì—¬ ë²¡í„° í¬ê¸°ë¥¼ 1ë¡œ ë§ì¶¤
   from sklearn.preprocessing import normalize
   embeddings = normalize(embeddings, norm='l2')
   ```

2. **ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ**:
   ```python
   # Sentence-BERTëŠ” ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
   embeddings = model.encode(titles, batch_size=64, show_progress_bar=True)
   ```

3. **GPU í™œìš©**:
   ```python
   # GPUê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš©
   model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')
   ```

4. **ì°¨ì› ì¡°ì •**:
   ```python
   # BERT4Rec ëª¨ë¸ì—ì„œ title ì„ë² ë”©ì€ Linearë¡œ hidden_dimì— ë§ì¶°ì§
   # title_projection: Linear(title_emb_dim, hidden_units)
   # ë”°ë¼ì„œ ì„ë² ë”© ì°¨ì›ì€ 384, 768 ë“± ììœ ë¡­ê²Œ ì„ íƒ ê°€ëŠ¥
   ```

### ë©”íƒ€ë°ì´í„° ìœµí•© ì „ëµ

ì„¸ ê°€ì§€ ìœµí•© ë°©ë²•ì„ ì§€ì›í•©ë‹ˆë‹¤:

#### 1. Concatenation (`metadata_fusion: "concat"`)

```
item_emb + genre_emb + director_emb + ... â†’ [hidden_dim * N]
        â†“
   Linear Layer
        â†“
   [hidden_dim]
```

- ëª¨ë“  ì„ë² ë”©ì„ ì—°ê²°í•˜ì—¬ Linear layerë¡œ ì¶•ì†Œ
- ë‹¨ìˆœí•˜ì§€ë§Œ íš¨ê³¼ì 
- ëª¨ë“  ë©”íƒ€ë°ì´í„°ê°€ ë™ë“±í•œ ê°€ì¤‘ì¹˜

#### 2. Addition (`metadata_fusion: "add"`)

```
item_emb + genre_emb + director_emb + ... = final_emb
```

- Element-wise ë§ì…ˆìœ¼ë¡œ ìœµí•©
- íŒŒë¼ë¯¸í„° ìˆ˜ ì¦ê°€ ì—†ìŒ
- ë©”íƒ€ë°ì´í„° ê°„ ê°€ì¤‘ì¹˜ ì¡°ì ˆ ë¶ˆê°€

#### 3. Gate Fusion (`metadata_fusion: "gate"`) â­ ê¶Œì¥

```
features = [item_emb, genre_emb, director_emb, writer_emb, title_emb]
gates = softmax(Linear(concat(features)))  # [N] (í•©ì´ 1)
        â†“
final_emb = Î£ (gate_i * feature_i)
```

- **í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜**ë¡œ ê° ë©”íƒ€ë°ì´í„°ì˜ ì¤‘ìš”ë„ ìë™ ì¡°ì ˆ
- Softmaxë¡œ ì •ê·œí™” (í•©ì´ 1)
- TensorBoardë¡œ gate ê°’ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥ (ì•„ë˜ ì„¹ì…˜ ì°¸ê³ )

**ì˜ˆì‹œ**:
```
Gate values: item=0.54, title=0.36, writer=0.06, director=0.02, genre=0.01
â†’ itemê³¼ titleì´ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ í•™ìŠµë¨
```

### ì„¤ì • ë°©ë²•

[bert4rec_v2.yaml](../configs/bert4rec_v2.yaml)ì—ì„œ ë©”íƒ€ë°ì´í„° ì˜µì…˜ ì„¤ì •:

```yaml
model:
  # ì‚¬ìš©í•  ë©”íƒ€ë°ì´í„° ì„ íƒ
  use_genre_emb: true       # ì¥ë¥´
  use_director_emb: true    # ê°ë…
  use_writer_emb: true      # ì‘ê°€
  use_title_emb: true       # ì œëª©

  # ìœµí•© ë°©ë²• ì„ íƒ
  metadata_fusion: "gate"   # "concat", "add", "gate" ì¤‘ ì„ íƒ
  metadata_dropout: 0.1     # Dropout ë¹„ìœ¨
```

**í•„ìš”í•œ ë°ì´í„° íŒŒì¼**:
- `~/data/train/genres.tsv`
- `~/data/train/directors.tsv`
- `~/data/train/writers.tsv`
- `~/data/train/titles.tsv`

íŒŒì¼ì´ ì—†ìœ¼ë©´ í•´ë‹¹ ë©”íƒ€ë°ì´í„°ëŠ” ìë™ìœ¼ë¡œ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.

### ë©”íƒ€ë°ì´í„° ì—†ì´ ì‚¬ìš©

ê¸°ë³¸ BERT4Rec (ë…¼ë¬¸ êµ¬í˜„)ë§Œ ì‚¬ìš©í•˜ë ¤ë©´:

```yaml
model:
  use_genre_emb: false
  use_director_emb: false
  use_writer_emb: false
  use_title_emb: false
```

ë˜ëŠ” ê°„ë‹¨íˆ ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì œê³µí•˜ì§€ ì•Šìœ¼ë©´ ë©ë‹ˆë‹¤.

## Gate Fusion ëª¨ë‹ˆí„°ë§ (TensorBoard)

Gate fusion ë°©ë²•ì„ ì‚¬ìš©í•  ë•Œ, ê° ë©”íƒ€ë°ì´í„°ì˜ ì¤‘ìš”ë„ë¥¼ TensorBoardë¡œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì‚¬ìš© ë°©ë²•

#### 1. Gate fusionìœ¼ë¡œ í•™ìŠµ ì‹œì‘

```bash
python train_bert4rec.py model.metadata_fusion=gate
```

#### 2. TensorBoard ì‹¤í–‰

```bash
# í•™ìŠµ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰
tensorboard --logdir=saved/tensorboard_logs/bert4rec
```

ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:6006` ì ‘ì†

#### 3. Gate ê°’ í™•ì¸

TensorBoardì—ì„œ ë‹¤ìŒ ë©”íŠ¸ë¦­ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

**SCALARS íƒ­**:
- `val_gate/item`: Item ì„ë² ë”©ì˜ ê°€ì¤‘ì¹˜
- `val_gate/genre`: Genre ì„ë² ë”©ì˜ ê°€ì¤‘ì¹˜
- `val_gate/director`: Director ì„ë² ë”©ì˜ ê°€ì¤‘ì¹˜
- `val_gate/writer`: Writer ì„ë² ë”©ì˜ ê°€ì¤‘ì¹˜
- `val_gate/title`: Title ì„ë² ë”©ì˜ ê°€ì¤‘ì¹˜

### Gate ê°’ í•´ì„

Gate ê°’ì€ **softmaxë¡œ ì •ê·œí™”**ë˜ì–´ í•©ì´ 1ì…ë‹ˆë‹¤:

```
Epoch 50:
  val_gate/item     = 0.54  (54%)
  val_gate/title    = 0.36  (36%)
  val_gate/writer   = 0.06  (6%)
  val_gate/director = 0.02  (2%)
  val_gate/genre    = 0.01  (1%)
```

**í•´ì„**:
- **Item ì„ë² ë”©(54%)**: í˜‘ì—… í•„í„°ë§ ì‹ í˜¸ê°€ ê°€ì¥ ì¤‘ìš”
- **Title ì„ë² ë”©(36%)**: ì½˜í…ì¸  ê¸°ë°˜ ì‹ í˜¸ (ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„±)ê°€ ë‘ ë²ˆì§¸ë¡œ ì¤‘ìš”
- **ê¸°íƒ€ ë©”íƒ€ë°ì´í„°(10%)**: ë³´ì¡°ì ì¸ ì—­í• 

### í™œìš© ì‚¬ë¡€

#### 1. ë¶ˆí•„ìš”í•œ ë©”íƒ€ë°ì´í„° ì œê±°

íŠ¹ì • ë©”íƒ€ë°ì´í„°ì˜ gate ê°’ì´ ì§€ì†ì ìœ¼ë¡œ ë‚®ë‹¤ë©´ (< 0.05), í•´ë‹¹ ë©”íƒ€ë°ì´í„°ë¥¼ ë¹„í™œì„±í™”í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```yaml
# Genreì˜ gate ê°’ì´ ê³„ì† 0.01ì´ë¼ë©´
model:
  use_genre_emb: false  # ë¹„í™œì„±í™”
```

#### 2. ì¤‘ìš” ë©”íƒ€ë°ì´í„° ì§‘ì¤‘

Gate ê°’ì´ ë†’ì€ ë©”íƒ€ë°ì´í„°ì˜ í’ˆì§ˆì„ ê°œì„ í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```
Title gateê°€ 0.36ìœ¼ë¡œ ë†’ìŒ
â†’ Title ì„ë² ë”© í’ˆì§ˆ ê°œì„  (ë” ë‚˜ì€ ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì‚¬ìš©)
```

#### 3. í•™ìŠµ ì•ˆì •ì„± í™•ì¸

Gate ê°’ì´ epochë§ˆë‹¤ í¬ê²Œ ë³€ë™í•˜ë©´ í•™ìŠµì´ ë¶ˆì•ˆì •í•œ ì‹ í˜¸ì…ë‹ˆë‹¤:

```yaml
# Learning rate ê°ì†Œ ë˜ëŠ” dropout ì¡°ì •
training:
  lr: 0.0005  # ë‚®ì¶¤
model:
  metadata_dropout: 0.2  # ì¦ê°€
```

### TensorBoard í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì 

ëª¨ë“  ì„¤ì • íŒŒë¼ë¯¸í„°ê°€ **HPARAMS íƒ­**ì— ìë™ìœ¼ë¡œ ê¸°ë¡ë©ë‹ˆë‹¤:

**Data ì„¤ì •**:
- `data/batch_size`
- `data/use_full_data`
- `data/seed`

**Model ì„¤ì •**:
- `model/hidden_units`
- `model/num_layers`
- `model/metadata_fusion`
- `model/use_genre_emb`, `model/use_director_emb`, ë“±

**Training ì„¤ì •**:
- `training/lr`
- `training/num_epochs`
- `training/early_stopping_patience`

**Metadata ì°¨ì›**:
- `metadata/num_genres`
- `metadata/num_directors`
- `metadata/num_items`

ì´ë¥¼ í†µí•´ ì—¬ëŸ¬ ì‹¤í—˜ì„ ë¹„êµí•˜ê³  ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ëª¨ë¸ ì•„í‚¤í…ì²˜

### BERT4Rec êµ¬ì¡° (ë…¼ë¬¸ Figure 2 ê¸°ì¤€)

```
Input Sequence: [itemâ‚, itemâ‚‚, [MASK], itemâ‚„, itemâ‚…]
    â†“
[Item Embedding] + [Position Embedding]
    â†“
    Dropout + LayerNorm
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformer Block 1            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Multi-Head Attention     â”‚   â”‚
â”‚  â”‚ (Bidirectional)          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚          â†“                      â”‚
â”‚    Residual + LayerNorm         â”‚
â”‚          â†“                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Feed-Forward Network     â”‚   â”‚
â”‚  â”‚ (4x expansion, GELU)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚          â†“                      â”‚
â”‚    Residual + LayerNorm         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
    ... (repeat N times)
    â†“
Output Projection (shared with item embedding)
    â†“
Predictions: [logits for all items]
```

### Transformer Block ìƒì„¸

**Multi-Head Attention**:
```
Q, K, V = Linear(x)
Q, K, V = split_heads(Q, K, V)  # [batch, num_heads, seq_len, head_dim]
attn = softmax(QÂ·K^T / âˆšhead_dim)  # Scaled Dot-Product
output = attn Â· V
output = concat_heads(output)
output = Linear(output)
```

**Feed-Forward Network**:
```
FFN(x) = Wâ‚‚(GELU(Wâ‚(x)))
where Wâ‚: hidden â†’ 4*hidden
      Wâ‚‚: 4*hidden â†’ hidden
```

### Special Tokens

- `0`: Padding token
- `1 ~ num_items`: Item indices
- `num_items + 1`: [MASK] token

### Masking Strategy (ë…¼ë¬¸ Section 3.2)

í•™ìŠµ ì‹œ ë‘ ê°€ì§€ ë§ˆìŠ¤í‚¹ ì „ëµì„ í˜¼í•©í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤:

#### 1. Random Masking (ê¸°ë³¸ ì „ëµ)

ê° ì•„ì´í…œì€ **`random_mask_prob` í™•ë¥ **ë¡œ ëœë¤í•˜ê²Œ ë§ˆìŠ¤í‚¹:
- **80%**: `[MASK]` í† í°ìœ¼ë¡œ ëŒ€ì²´
- **10%**: ëœë¤ ì•„ì´í…œìœ¼ë¡œ ëŒ€ì²´
- **10%**: ì›ë³¸ ìœ ì§€

#### 2. Last Item Masking Boost (ì¶”ê°€ ì „ëµ)

**ëª¨ë“  ìƒ˜í”Œ**ì— ëœë¤ ë§ˆìŠ¤í‚¹ì„ ì ìš©í•œ í›„, **`last_item_mask_ratio` í™•ë¥ **ë¡œ ë§ˆì§€ë§‰ ì•„ì´í…œì„ ì¶”ê°€ë¡œ ë§ˆìŠ¤í‚¹:
- ëœë¤ ë§ˆìŠ¤í‚¹ìœ¼ë¡œ ë‹¤ì–‘í•œ íŒ¨í„´ í•™ìŠµ (ë§¤ epoch ë‹¤ë¥¸ mask pattern)
- ì¶”ê°€ë¡œ ë§ˆì§€ë§‰ ì•„ì´í…œì„ ê°•ì œ ë§ˆìŠ¤í‚¹í•˜ì—¬ next-item prediction ê°•í™”
- ì¶”ë¡  ì‹œë‚˜ë¦¬ì˜¤ì™€ ì¼ì¹˜í•˜ëŠ” íŒ¨í„´ í•™ìŠµ

**ì„¤ì • ì˜ˆì‹œ** ([bert4rec_v2.yaml](../configs/bert4rec_v2.yaml)):
```yaml
model:
  random_mask_prob: 0.2        # ëœë¤ ë§ˆìŠ¤í‚¹ í™•ë¥ 
  last_item_mask_ratio: 0.05   # ëœë¤ ë§ˆìŠ¤í‚¹ í›„ ì¶”ê°€ë¡œ ë§ˆì§€ë§‰ ì•„ì´í…œì„ ë§ˆìŠ¤í‚¹í•  í™•ë¥ 
```

**ë™ì‘ ë°©ì‹** (ê°œì„ ë¨):
```
ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•´:
1. Random masking ì ìš© (20% í™•ë¥ ë¡œ ê° ì•„ì´í…œ)
2. 5% í™•ë¥ ë¡œ ë§ˆì§€ë§‰ ì•„ì´í…œì„ ì¶”ê°€ë¡œ ë§ˆìŠ¤í‚¹

ë§¤ epochë§ˆë‹¤ ë‹¤ë¥¸ mask pattern ìƒì„± â†’ ë°ì´í„° ë‹¤ì–‘ì„± í–¥ìƒ
```

**ì¥ì **:
- âœ… **ë§¤ epoch ë‹¤ì–‘í•œ í•™ìŠµ ë°ì´í„°** (ê¸°ì¡´: ê°™ì€ last-item mask ë°˜ë³µ)
- âœ… **Random masking + Last-item masking íš¨ê³¼ ë™ì‹œ íšë“**
- âœ… ì¶”ë¡  ì‹œì™€ ë™ì¼í•œ ë§ˆìŠ¤í‚¹ íŒ¨í„´ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
- âœ… ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì•„ì´í…œ ì˜ˆì¸¡ ëŠ¥ë ¥ ê°•í™”

**êµ¬í˜„ ìƒì„¸** ([bert4rec_data.py:81-90](../src/data/bert4rec_data.py#L81-L90)):
```python
# Always apply random masking first (for data diversity)
tokens, labels = self._random_mask_sequence(seq)

# Additionally mask the last item with probability last_item_mask_ratio
if len(seq) > 0 and np.random.random() < self.last_item_mask_ratio:
    # Force mask the last item (overwrite if already masked)
    last_idx = len(seq) - 1
    tokens[last_idx] = self.mask_token
    labels[last_idx] = seq[last_idx]  # Original item as label
```

**ì¶”ë¡  ì‹œ**:
- ë§ˆì§€ë§‰ ìœ„ì¹˜ì— `[MASK]` ì¶”ê°€
- ëª¨ë¸ì´ ë‹¤ìŒ ì•„ì´í…œ ì˜ˆì¸¡

## í‰ê°€ ë©”íŠ¸ë¦­

### HIT@K (Hit Ratio)
```
HIT@K = 1 if ground_truth in top_K else 0
```

### NDCG@K (Normalized Discounted Cumulative Gain)
```
NDCG@K = 1 / logâ‚‚(rank + 2)  if rank < K else 0
```

### MRR (Mean Reciprocal Rank) - ë¯¸êµ¬í˜„
```
MRR = 1 / rank
```

## í•™ìŠµ ëª¨ë“œ

### Standard ëª¨ë“œ (ê¸°ë³¸ê°’: `use_full_data: false`)

```bash
python train_bert4rec.py
```

**íŠ¹ì§•**:
- âœ… Train/Validation split ìë™ ìƒì„± (ë§ˆì§€ë§‰ ì•„ì´í…œ)
- âœ… Validation ë©”íŠ¸ë¦­ ê³„ì‚° (Hit@10, NDCG@10)
- âœ… Early stopping í™œì„±í™”
- âœ… Checkpoint: `val_ndcg@10` ê¸°ì¤€ ì €ì¥
- âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œ ê¶Œì¥

**ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ëª…**: `bert4rec-{epoch:02d}-{val_ndcg@10:.4f}.ckpt`

### Full Data ëª¨ë“œ (`use_full_data: true`)

```bash
python train_bert4rec.py data.use_full_data=true
```

**íŠ¹ì§•**:
- âœ… ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ (validation split ì—†ìŒ)
- âœ… Early stopping ìë™ ë¹„í™œì„±í™”
- âœ… Checkpoint: `train_loss` ê¸°ì¤€ ì €ì¥
- âœ… ìµœì¢… ì œì¶œìš© ëª¨ë¸ í•™ìŠµ ì‹œ ê¶Œì¥
- âš ï¸ Overfitting í™•ì¸ ë¶ˆê°€ (epoch ìˆ˜ ì¡°ì ˆ í•„ìš”)

**ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ëª…**: `bert4rec-{epoch:02d}-{train_loss:.4f}.ckpt`

**ìë™ ì¡°ì • ì‚¬í•­**:
| ì„¤ì • | Standard ëª¨ë“œ | Full Data ëª¨ë“œ |
|------|--------------|----------------|
| Train data | `seq[:-1]` | `seq` (ì „ì²´) |
| Validation | í™œì„±í™” | ë¹„í™œì„±í™” |
| Early stopping | í™œì„±í™” | ë¹„í™œì„±í™” |
| Checkpoint metric | `val_ndcg@10` | `train_loss` |
| Checkpoint mode | `max` | `min` |

## PyTorch Lightning ê¸°ëŠ¥

### ìë™ ì œê³µë˜ëŠ” ê¸°ëŠ¥

1. **ë¶„ì‚° í•™ìŠµ**: Multi-GPU, TPU ìë™ ì§€ì›
2. **ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬**:
   - `last.ckpt`: ë§ˆì§€ë§‰ epoch
   - `best.ckpt`: ìµœê³  ì„±ëŠ¥ ëª¨ë¸
3. **Early Stopping**: Validation ì„±ëŠ¥ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨ (Standard ëª¨ë“œ)
4. **ë¡œê¹…**: TensorBoard ìë™ ë¡œê¹…
5. **ì¬í˜„ì„±**: Seed ì„¤ì •ìœ¼ë¡œ ì¬í˜„ ê°€ëŠ¥

### ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ

```python
from src.models.bert4rec import BERT4Rec

# ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ
model = BERT4Rec.load_from_checkpoint("path/to/checkpoint.ckpt")
model.eval()

# ì¶”ë¡ 
predictions = model.predict(
    user_sequences=[[1, 2, 3, 4], [5, 6, 7]],
    topk=10,
    exclude_items=[set([1,2,3,4]), set([5,6,7])]
)
```

## ë…¼ë¬¸ êµ¬í˜„ ì‚¬í•­

### âœ… ë…¼ë¬¸ê³¼ ë™ì¼í•˜ê²Œ êµ¬í˜„ëœ ë¶€ë¶„

| êµ¬ì„± ìš”ì†Œ | ë…¼ë¬¸ ëª…ì„¸ | êµ¬í˜„ ìƒíƒœ |
|-----------|-----------|-----------|
| **Architecture** | Bidirectional Transformer | âœ… êµ¬í˜„ |
| **Attention** | Multi-Head Self-Attention | âœ… êµ¬í˜„ |
| **Scaling** | QÂ·K^T / âˆšd_k (d_k = head_dim) | âœ… êµ¬í˜„ |
| **FFN** | 4x hidden dimension expansion | âœ… êµ¬í˜„ |
| **Activation** | GELU | âœ… êµ¬í˜„ |
| **Normalization** | Layer Normalization | âœ… êµ¬í˜„ |
| **Residual** | Residual Connection | âœ… êµ¬í˜„ |
| **Position Encoding** | Learnable positional embeddings | âœ… êµ¬í˜„ |
| **Masking Strategy** | 15% masking (80% [MASK], 10% random, 10% keep) | âœ… êµ¬í˜„ |
| **Loss** | Cross-Entropy on masked positions | âœ… êµ¬í˜„ |
| **Padding** | Zero padding with padding_idx=0 | âœ… êµ¬í˜„ |

### ğŸ“ ë…¼ë¬¸ê³¼ ë‹¤ë¥´ê±°ë‚˜ ì„ íƒ ê°€ëŠ¥í•œ ë¶€ë¶„

#### 1. Output Layer Weight Sharing

**ë…¼ë¬¸**:
> "we tie the item embedding matrix with the final output projection matrix to reduce parameters"

**êµ¬í˜„**:
```python
share_embeddings: bool = True  # ê¸°ë³¸ê°’: True (ë…¼ë¬¸ê³¼ ë™ì¼)
```

- âœ… **True** (ê¸°ë³¸ê°’): ë…¼ë¬¸ê³¼ ë™ì¼, item embedding ì¬ì‚¬ìš©
- âŒ **False**: ë³„ë„ì˜ Linear layer ì‚¬ìš© (ë” ë§ì€ íŒŒë¼ë¯¸í„°)

**ì„¤ì • ìœ„ì¹˜**: `configs/bert4rec.yaml`
```yaml
model:
  share_embeddings: true  # ê¶Œì¥ (ë…¼ë¬¸ê³¼ ë™ì¼)
```

#### 2. Validation ì „ëµ

**ë…¼ë¬¸**:
> "we randomly select 100 negative items and rank these 101 items"

**êµ¬í˜„**:
- **ì „ì²´ ì•„ì´í…œ ranking** ë°©ì‹ êµ¬í˜„ (ë” ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
- ìƒ˜í”Œë§ ê¸°ë°˜ í‰ê°€ëŠ” ë¯¸êµ¬í˜„

**ì„ íƒ ì´ìœ **:
- ì •í™•í•œ ë©”íŠ¸ë¦­ ì¸¡ì •ì„ ìœ„í•´ ì „ì²´ ranking ì„ íƒ
- í•„ìš”ì‹œ ìƒ˜í”Œë§ ë°©ì‹ ì¶”ê°€ ê°€ëŠ¥

#### 3. Optimizer

**ë…¼ë¬¸**:
> "We use Adam optimizer with learning rate of 0.001"

**êµ¬í˜„**:
```python
optimizer = torch.optim.Adam(
    self.parameters(),
    lr=self.lr,  # ê¸°ë³¸ê°’: 0.0015
    weight_decay=self.weight_decay  # ê¸°ë³¸ê°’: 0.0
)
```

**ì°¨ì´ì **:
- ë…¼ë¬¸ ê¸°ë³¸ê°’: `lr=0.001`
- êµ¬í˜„ ê¸°ë³¸ê°’: `lr=0.0015` (ì‹¤í—˜ì ìœ¼ë¡œ ì¡°ì • ê°€ëŠ¥)

#### 4. Learning Rate Scheduler

**ë…¼ë¬¸**:
> No explicit mention of learning rate scheduling

**êµ¬í˜„**:
- í˜„ì¬ ë¯¸êµ¬í˜„ (constant learning rate)
- Lightningì˜ `configure_optimizers()`ì—ì„œ ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥

**ì¶”ê°€ ë°©ë²•**:
```python
def configure_optimizers(self):
    optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)
    return [optimizer], [scheduler]
```

#### 5. Dropout ìœ„ì¹˜

**ë…¼ë¬¸**:
> "we apply dropout on all intermediate layers including the embedding layer"

**êµ¬í˜„**:
- âœ… Embedding layer ì´í›„
- âœ… Attention distribution
- âœ… Feed-forward network
- âœ… Residual connection ì´í›„

**ëª¨ë‘ ë…¼ë¬¸ê³¼ ë™ì¼**

#### 6. Attention Mask

**ë…¼ë¬¸**:
> "bidirectional model... each position can attend to all positions"

**êµ¬í˜„**:
```python
# Paddingë§Œ ë§ˆìŠ¤í‚¹ (bidirectional attention ìœ ì§€)
mask = (log_seqs > 0).unsqueeze(1).unsqueeze(2)
mask = mask.expand(-1, -1, seq_len, -1)  # [batch, 1, seq_len, seq_len]
```

**ë…¼ë¬¸ê³¼ ë™ì¼**: íŒ¨ë”© ìœ„ì¹˜ë§Œ ë§ˆìŠ¤í‚¹, ëª¨ë“  ìœ íš¨í•œ ìœ„ì¹˜ëŠ” ì„œë¡œ ë³¼ ìˆ˜ ìˆìŒ

#### 7. í‰ê°€ ë©”íŠ¸ë¦­

**ë…¼ë¬¸**:
> "HIT@K, NDCG@K, MRR"

**êµ¬í˜„**:
- âœ… HIT@10
- âœ… NDCG@10
- âŒ MRR (ë¯¸êµ¬í˜„)

**MRR ì¶”ê°€ ê°€ëŠ¥**:
```python
# validation_stepì— ì¶”ê°€
mrr = 1.0 / (rank + 1)  # rankëŠ” 0-based
self.log('val_mrr', mrr, ...)
```

## ì„±ëŠ¥ ìµœì í™” íŒ

### 1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ

```yaml
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
data:
  batch_size: 64

# í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•´ 32-bit precision ì‚¬ìš© (ë˜ëŠ” "16-mixed"ë¡œ ì†ë„ í–¥ìƒ)
training:
  precision: "32-true"
```

### 2. í•™ìŠµ ì†ë„ í–¥ìƒ

```yaml
# DataLoader workers ì¦ê°€
data:
  num_workers: 8

# ê²€ì¦ ë¹ˆë„ ì¤„ì´ê¸° (2 epochë§ˆë‹¤)
training:
  val_check_interval: 2.0
```

### 3. ë°ì´í„°ì…‹ë³„ ê¶Œì¥ ì„¤ì •

**ì‘ì€ ë°ì´í„°ì…‹** (< 10K users):
```yaml
model:
  hidden_units: 32
  num_heads: 2
  num_layers: 2
  max_len: 30
```

**ì¤‘ê°„ ë°ì´í„°ì…‹** (10K ~ 100K users):
```yaml
model:
  hidden_units: 64
  num_heads: 4
  num_layers: 3
  max_len: 50
```

**í° ë°ì´í„°ì…‹** (> 100K users):
```yaml
model:
  hidden_units: 128
  num_heads: 8
  num_layers: 4
  max_len: 200
```

## ë…¼ë¬¸ ì¬í˜„ì„ ìœ„í•œ ì„¤ì •

### MovieLens-1M (ë…¼ë¬¸ Table 2)

```yaml
model:
  hidden_units: 256
  num_heads: 4
  num_layers: 2
  max_len: 200
  dropout_rate: 0.2
  mask_prob: 0.15

training:
  lr: 0.001
  num_epochs: 200
```

### Steam (ë…¼ë¬¸ Table 2)

```yaml
model:
  hidden_units: 256
  num_heads: 4
  num_layers: 2
  max_len: 50
  dropout_rate: 0.5
  mask_prob: 0.15

training:
  lr: 0.001
  num_epochs: 200
```

## ë¬¸ì œ í•´ê²°

### Import Error

```bash
# PYTHONPATH ì„¤ì •
export PYTHONPATH=/data/ephemeral/home/juik/lightning:$PYTHONPATH
```

### CUDA Out of Memory

```yaml
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
data:
  batch_size: 64

# ì‹œí€€ìŠ¤ ê¸¸ì´ ì¤„ì´ê¸°
model:
  max_len: 30

# í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•´ 32-bit precision ì‚¬ìš© (ë˜ëŠ” "16-mixed"ë¡œ ì†ë„ í–¥ìƒ)
training:
  precision: "32-true"
```

### í•™ìŠµì´ ëŠë¦° ê²½ìš°

```yaml
# 1. Workers ì¦ê°€
data:
  num_workers: 8

# 2. ê²€ì¦ ë¹ˆë„ ì¤„ì´ê¸°
training:
  val_check_interval: 2.0

# 3. Pin memory í™œì„±í™” (DataLoaderì—ì„œ ìë™)
```

### Validation ì„±ëŠ¥ì´ ë‚®ì€ ê²½ìš°

1. **Masking í™•ë¥  ì¡°ì •**:
   ```yaml
   model:
     mask_prob: 0.2  # 0.15ì—ì„œ ì¦ê°€
   ```

2. **Dropout ì¤„ì´ê¸°**:
   ```yaml
   model:
     dropout_rate: 0.2  # 0.3ì—ì„œ ê°ì†Œ
   ```

3. **ëª¨ë¸ í¬ê¸° ì¦ê°€**:
   ```yaml
   model:
     hidden_units: 128
     num_layers: 4
   ```

## êµ¬í˜„ ì„¸ë¶€ ì‚¬í•­

### Weight Initialization (ë…¼ë¬¸ ëª…ì‹œ ì•ˆë¨)

```python
# Normal distribution (mean=0, std=0.02)
nn.init.normal_(module.weight, mean=0.0, std=0.02)

# Padding embeddingì€ 0ìœ¼ë¡œ ì´ˆê¸°í™”
if module.padding_idx is not None:
    nn.init.zeros_(module.weight[module.padding_idx])
```

### Loss Computation

```python
# Cross-entropy on masked positions only
criterion = nn.CrossEntropyLoss(ignore_index=0)

# ignore_index=0: íŒ¨ë”© ë° ë¹„ë§ˆìŠ¤í‚¹ ìœ„ì¹˜ ë¬´ì‹œ
```

### Attention Mask Shape

```python
# Input: [batch, seq_len]
# Mask: [batch, 1, seq_len, seq_len]
#
# mask[b, 0, i, j] = 1 if position j is valid (not padding)
#                  = 0 if position j is padding
```

## Future Information Leakage ë°©ì§€

### ê°œìš”

ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ **ì‹œê°„ì  ì •ë³´ ìœ ì¶œ(Future Information Leakage)** ë¬¸ì œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´, ì‚¬ìš©ìê°€ ì•„ì§ ì•Œ ìˆ˜ ì—†ëŠ” ë¯¸ë˜ ì •ë³´(ê°œë´‰ ì „ ì˜í™”)ë¥¼ ì¶”ì²œì—ì„œ ìë™ìœ¼ë¡œ ì œì™¸í•©ë‹ˆë‹¤.

### ë™ì‘ ì›ë¦¬

```
ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ í´ë¦­: 2018ë…„ 5ì›”
â†“
ê°œë´‰ë…„ë„ê°€ 2019ë…„ ì´í›„ì¸ ì˜í™”ë“¤ì€ ì¶”ì²œì—ì„œ ì œì™¸
(ì‚¬ìš©ìê°€ 2018ë…„ì— 2019ë…„ ê°œë´‰ ì˜í™”ë¥¼ ì•Œ ìˆ˜ ì—†ìŒ)
```

### êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

#### 1. ë°ì´í„° ë¡œë”© ([src/data/bert4rec_data.py](../src/data/bert4rec_data.py))

```python
# years.tsvì—ì„œ ì˜í™” ê°œë´‰ë…„ë„ ë¡œë“œ
self.item_years = {}  # Dict[item_idx, year]

# ì‚¬ìš©ìë³„ ë§ˆì§€ë§‰ í´ë¦­ ë…„ë„ ê³„ì‚°
self.user_last_click_years = {}  # Dict[user_idx, year]
```

**í•„ìš”í•œ íŒŒì¼**:
- `years.tsv`: ì˜í™” IDì™€ ê°œë´‰ë…„ë„ ë§¤í•‘ (íƒ­ êµ¬ë¶„)
- `train_ratings.csv`: `time` ì»¬ëŸ¼ í•„ìˆ˜ (Unix timestamp)

#### 2. Future Items í•„í„°ë§

```python
def get_future_item_sequences(self):
    """
    ê° ìœ ì €ë³„ë¡œ ì¶”ì²œì—ì„œ ì œì™¸í•  future items ë°˜í™˜

    Returns:
        Dict[user_idx, Set[item_idx]]
    """
    future_items = {}
    for user_idx in users:
        last_click_year = self.user_last_click_years[user_idx]
        # ê°œë´‰ë…„ë„ > ë§ˆì§€ë§‰ í´ë¦­ ë…„ë„ì¸ ì˜í™” í•„í„°ë§
        future_items[user_idx] = {
            item_idx for item_idx in items
            if self.item_years[item_idx] > last_click_year
        }
    return future_items
```

#### 3. ì¶”ë¡  ì‹œ ìë™ ì ìš© ([predict_bert4rec.py](../predict_bert4rec.py))

```python
# Future items ê°€ì ¸ì˜¤ê¸°
future_item_sequences = datamodule.get_future_item_sequences()

# ì¶”ì²œì—ì„œ ì œì™¸ (train + valid + future items)
for user_idx in batch_users:
    exclude_set = set(full_seq)  # ì´ë¯¸ ë³¸ ì˜í™”
    exclude_set.update(future_item_sequences[user_idx])  # Future ì˜í™”

    top_items = model.predict(
        user_sequences=batch_seqs,
        topk=topk,
        exclude_items=exclude_set  # ì œì™¸ ëª©ë¡
    )
```

### ë¡œê·¸ ì˜ˆì‹œ

```
[INFO] Loading item metadata...
[INFO] Loaded 6807 items with release year info
[INFO] Calculated last click year for 31360 users
[INFO] Future items to filter: 289456 items across 28934 users
```

### ì˜ˆì‹œ

**ì‚¬ìš©ì A**:
- ë§ˆì§€ë§‰ í´ë¦­: `2018-05-15` â†’ ë…„ë„: `2018`
- ì¶”ì²œ í›„ë³´: `[ì–´ë²¤ì ¸ìŠ¤: ì—”ë“œê²Œì„ (2019), ê²¨ìš¸ì™•êµ­ 2 (2019), ...]`
- **ê²°ê³¼**: 2019ë…„ ì´í›„ ê°œë´‰ ì˜í™” ëª¨ë‘ ì œì™¸ âœ…

**ì‚¬ìš©ì B**:
- ë§ˆì§€ë§‰ í´ë¦­: `2020-12-01` â†’ ë…„ë„: `2020`
- ì¶”ì²œ í›„ë³´: `[ì–´ë²¤ì ¸ìŠ¤: ì—”ë“œê²Œì„ (2019), ê²¨ìš¸ì™•êµ­ 2 (2019), ...]`
- **ê²°ê³¼**: 2019ë…„ ì˜í™”ëŠ” ì¶”ì²œ ê°€ëŠ¥ âœ…

### ë¹„í™œì„±í™” ë°©ë²•

Future information leakage ë°©ì§€ë¥¼ ë¹„í™œì„±í™”í•˜ë ¤ë©´:

```python
# predict_bert4rec.py ìˆ˜ì •
future_item_sequences = datamodule.get_future_item_sequences()

# ëª¨ë“  ìœ ì €ì— ëŒ€í•´ ë¹ˆ setìœ¼ë¡œ ë³€ê²½
future_item_sequences = {u: set() for u in range(datamodule.num_users)}
```

### ë°ì´í„° ìš”êµ¬ì‚¬í•­

1. **years.tsv** (í•„ìˆ˜):
   ```tsv
   item	year
   1	1995
   2	1995
   3	1995
   ```

2. **train_ratings.csv** (time ì»¬ëŸ¼ í•„ìˆ˜):
   ```csv
   user,item,time
   1,31,1260759144
   1,1029,1260759179
   ```

`time` ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ year filteringì´ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

## í…ŒìŠ¤íŠ¸ (pytest)

BERT4Rec ëª¨ë¸ì˜ í’ˆì§ˆì„ ë³´ì¥í•˜ê¸° ìœ„í•´ pytest ê¸°ë°˜ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •

```bash
# í…ŒìŠ¤íŠ¸ ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r lightning/requirements-dev.txt

# ë˜ëŠ” ê°œë³„ ì„¤ì¹˜
pip install pytest pytest-cov
```

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰

#### 1. ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
cd lightning
pytest
```

#### 2. íŠ¹ì • í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‹¤í–‰

```bash
# BERT4Rec ëª¨ë¸ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
pytest tests/unit/test_bert4rec_model.py

# íŠ¹ì • í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ë§Œ ì‹¤í–‰
pytest tests/unit/test_bert4rec_model.py::TestBERT4RecForwardPass

# íŠ¹ì • í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë§Œ ì‹¤í–‰
pytest tests/unit/test_bert4rec_model.py::TestBERT4RecForwardPass::test_forward_pass_without_metadata
```

#### 3. ìƒì„¸ ì¶œë ¥ ëª¨ë“œ

```bash
# ìƒì„¸ ë¡œê·¸ ì¶œë ¥
pytest -v

# ë” ìƒì„¸í•œ ì¶œë ¥ (í…ŒìŠ¤íŠ¸ ì§„í–‰ ìƒí™© í‘œì‹œ)
pytest -vv

# í‘œì¤€ ì¶œë ¥ë„ í‘œì‹œ (print ë¬¸ ì¶œë ¥)
pytest -s
```

#### 4. ì»¤ë²„ë¦¬ì§€ í™•ì¸

```bash
# ì»¤ë²„ë¦¬ì§€ì™€ í•¨ê»˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest --cov=src --cov-report=html

# HTML ë¦¬í¬íŠ¸ í™•ì¸
open htmlcov/index.html
```

#### 5. ë§ˆì»¤(Marker)ë¡œ í•„í„°ë§

```bash
# Unit í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
pytest -m unit

# GPU í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰ (GPU ìˆì„ ë•Œ)
pytest -m gpu

# GPU í…ŒìŠ¤íŠ¸ ì œì™¸
pytest -m "not gpu"
```

### í…ŒìŠ¤íŠ¸ êµ¬ì¡°

```
lightning/tests/
â”œâ”€â”€ conftest.py                    # ê³µí†µ fixtures (ìƒ˜í”Œ ë°ì´í„°, ëª¨ë¸ ì„¤ì •)
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_bert4rec_model.py     # ëª¨ë¸ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ TEST_SUMMARY.md            # í…ŒìŠ¤íŠ¸ ìš”ì•½ ë¬¸ì„œ
â””â”€â”€ integration/                    # í†µí•© í…ŒìŠ¤íŠ¸ (ì˜ˆì •)
```

### ì£¼ìš” í…ŒìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬

#### 1. ëª¨ë¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ (`TestBERT4RecModelInitialization`)

```python
# ëª¨ë¸ì´ ì˜¬ë°”ë¥´ê²Œ ìƒì„±ë˜ëŠ”ì§€ í™•ì¸
def test_model_creates_successfully()
def test_model_has_correct_attributes()
def test_special_tokens_initialized()
def test_metadata_embeddings_created()
```

#### 2. Forward Pass í…ŒìŠ¤íŠ¸ (`TestBERT4RecForwardPass`)

```python
# Forward passê°€ ì˜¬ë°”ë¥¸ í˜•íƒœì˜ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ”ì§€ í™•ì¸
def test_forward_pass_without_metadata()
def test_forward_pass_with_metadata()
def test_forward_output_dtype()
def test_forward_no_nan_or_inf()
```

#### 3. Metadata Fusion í…ŒìŠ¤íŠ¸ (`TestBERT4RecMetadataFusion`)

```python
# ì„¸ ê°€ì§€ ìœµí•© ì „ëµ í…ŒìŠ¤íŠ¸
def test_concat_fusion()
def test_add_fusion()
def test_gate_fusion()
def test_invalid_fusion_raises_error()
```

#### 4. Gate Values í…ŒìŠ¤íŠ¸ (`TestBERT4RecGateValues`)

```python
# Gate ê°’ì´ ì˜¬ë°”ë¥´ê²Œ ë°˜í™˜ë˜ê³  ì •ê·œí™”ë˜ëŠ”ì§€ í™•ì¸
def test_gate_fusion_returns_values()
def test_gate_values_sum_to_one()        # Softmax ê²€ì¦
def test_gate_values_are_positive()       # [0,1] ë²”ìœ„ ê²€ì¦
def test_gate_num_features_matches_enabled_embeddings()
```

#### 5. Training/Validation Step í…ŒìŠ¤íŠ¸

```python
# í•™ìŠµê³¼ ê²€ì¦ì´ ì˜¬ë°”ë¥´ê²Œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸
def test_training_step_without_metadata()
def test_training_step_with_metadata()
def test_validation_step_logs_gate_values()
```

#### 6. Edge Cases í…ŒìŠ¤íŠ¸ (`TestBERT4RecSpecialCases`)

```python
# ê·¹ë‹¨ì ì¸ ê²½ìš° ì²˜ë¦¬ í™•ì¸
def test_all_padding_sequence()
def test_all_mask_tokens()
def test_single_item_sequence()
def test_max_length_sequence()
```

### ìƒ˜í”Œ Fixtures (conftest.py)

í…ŒìŠ¤íŠ¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê³µí†µ fixtures:

```python
@pytest.fixture
def sample_config():
    """ê¸°ë³¸ BERT4Rec ì„¤ì •"""

@pytest.fixture
def sample_config_no_metadata():
    """ë©”íƒ€ë°ì´í„° ì—†ëŠ” ì„¤ì •"""

@pytest.fixture
def bert4rec_model():
    """ë©”íƒ€ë°ì´í„° í¬í•¨ ëª¨ë¸"""

@pytest.fixture
def bert4rec_model_no_metadata():
    """ë©”íƒ€ë°ì´í„° ì—†ëŠ” ëª¨ë¸"""

@pytest.fixture
def sample_batch():
    """ìƒ˜í”Œ ë°°ì¹˜ ë°ì´í„° (sequences, labels)"""

@pytest.fixture
def sample_metadata():
    """ìƒ˜í”Œ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬"""

@pytest.fixture
def temp_data_dir():
    """ì„ì‹œ ë°ì´í„° ë””ë ‰í† ë¦¬ (ìë™ ì •ë¦¬)"""
```

### ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ì¶”ê°€

ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì¶”ê°€í•  ë•Œ í…ŒìŠ¤íŠ¸ë„ í•¨ê»˜ ì‘ì„±í•˜ì„¸ìš”:

```python
# tests/unit/test_bert4rec_model.py

import pytest
from src.models.bert4rec import BERT4Rec

@pytest.mark.unit
class TestMyNewFeature:
    """ìƒˆë¡œìš´ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""

    def test_feature_works_correctly(self, bert4rec_model):
        """ê¸°ëŠ¥ì´ ì˜¬ë°”ë¥´ê²Œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸"""
        result = bert4rec_model.my_new_method()
        assert result is not None
        assert result.shape == (expected_shape)

    def test_feature_handles_edge_case(self):
        """ì—£ì§€ ì¼€ì´ìŠ¤ ì²˜ë¦¬ í™•ì¸"""
        # í…ŒìŠ¤íŠ¸ ì½”ë“œ
        pass
```

### CI/CD í†µí•©

GitHub Actions ë“±ì˜ CI/CD íŒŒì´í”„ë¼ì¸ì—ì„œ ìë™ í…ŒìŠ¤íŠ¸:

```yaml
# .github/workflows/test.yml
name: Tests
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
      - run: pip install -r requirements-dev.txt
      - run: pytest -v --cov=src
```

### í…ŒìŠ¤íŠ¸ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

1. **ì‘ì€ ë‹¨ìœ„ë¡œ í…ŒìŠ¤íŠ¸**: ê° í…ŒìŠ¤íŠ¸ëŠ” í•˜ë‚˜ì˜ ê¸°ëŠ¥ë§Œ ê²€ì¦
2. **ë…ë¦½ì„± ìœ ì§€**: í…ŒìŠ¤íŠ¸ ê°„ ì˜ì¡´ì„± ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥
3. **ë¹ ë¥¸ ì‹¤í–‰**: ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ëŠ” ë¹ ë¥´ê²Œ ì‹¤í–‰ë˜ì–´ì•¼ í•¨
4. **ëª…í™•í•œ ì´ë¦„**: í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì´ë¦„ë§Œìœ¼ë¡œ ë¬´ì—‡ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆê²Œ
5. **Fixtures í™œìš©**: ì¤‘ë³µ ì½”ë“œ ì œê±°ë¥¼ ìœ„í•´ ê³µí†µ ì„¤ì •ì€ fixturesë¡œ
6. **ì˜ˆì™¸ í…ŒìŠ¤íŠ¸**: ì •ìƒ ì¼€ì´ìŠ¤ë¿ë§Œ ì•„ë‹ˆë¼ ì˜ˆì™¸ ìƒí™©ë„ í…ŒìŠ¤íŠ¸

## ì°¸ê³  ìë£Œ

- [BERT4Rec ë…¼ë¬¸](https://arxiv.org/abs/1904.06690)
- [BERT ì›ë³¸ ë…¼ë¬¸](https://arxiv.org/abs/1810.04805)
- [PyTorch Lightning ë¬¸ì„œ](https://lightning.ai/docs/pytorch/stable/)
- [Hydra ë¬¸ì„œ](https://hydra.cc/)

## ë¼ì´ì„ ìŠ¤

MIT License

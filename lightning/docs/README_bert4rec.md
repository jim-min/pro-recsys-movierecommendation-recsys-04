# BERT4Rec Implementation Guide

## ê°œìš”

BERT4Rec (Bidirectional Encoder Representations from Transformers for Sequential Recommendation)ì˜ PyTorch Lightning êµ¬í˜„ì…ë‹ˆë‹¤.

**ë…¼ë¬¸**: [BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer](https://arxiv.org/abs/1904.06690)

## íŒŒì¼ êµ¬ì¡°

```
lightning/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ bert4rec.yaml              # Hydra ì„¤ì • íŒŒì¼
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ bert4rec.py            # BERT4Rec ëª¨ë¸ (LightningModule)
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ bert4rec_data.py       # DataModule
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ README_bert4rec.md         # ì´ ë¬¸ì„œ
â”œâ”€â”€ train_bert4rec.py              # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ predict_bert4rec.py            # ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ run_bert4rec.sh                # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
```

## ì‚¬ìš©ë²•

### 1. í•™ìŠµ

```bash
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ
python train_bert4rec.py

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì˜¤ë²„ë¼ì´ë“œ
python train_bert4rec.py model.hidden_units=128 training.lr=0.001

# ë…¼ë¬¸ ê¸°ì¤€ í•™ìŠµ (lr=0.001)
python train_bert4rec.py training.lr=0.001

# ì—¬ëŸ¬ íŒŒë¼ë¯¸í„° ë™ì‹œ ë³€ê²½
python train_bert4rec.py \
    model.hidden_units=128 \
    model.num_heads=8 \
    model.num_layers=4 \
    training.num_epochs=500 \
    training.lr=0.001
```

### 2. ì¶”ë¡ 

```bash
# ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ë¡œ ì¶”ë¡ 
python predict_bert4rec.py

# íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©
python predict_bert4rec.py inference.checkpoint_path=saved/bert4rec/checkpoints/best.ckpt

# Top-K ë³€ê²½
python predict_bert4rec.py inference.topk=20
```

### 3. ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹¤í–‰

```bash
# í•™ìŠµ + ì¶”ë¡ 
./run_bert4rec.sh both

# í•™ìŠµë§Œ
./run_bert4rec.sh train

# ì¶”ë¡ ë§Œ
./run_bert4rec.sh predict
```

## ì„¤ì • (configs/bert4rec.yaml)

### ë°ì´í„° ì„¤ì •

```yaml
data:
  data_dir: "~/data/train/"        # ë°ì´í„° ë””ë ‰í† ë¦¬
  data_file: "train_ratings.csv"   # CSV íŒŒì¼ëª…
  batch_size: 128                  # ë°°ì¹˜ í¬ê¸°
  min_interactions: 3              # ìµœì†Œ interaction ìˆ˜
  num_workers: 4                   # DataLoader workers
```

**ë°ì´í„° í¬ë§·**: CSV with columns `user`, `item`, `time` (optional)

### ëª¨ë¸ ì„¤ì •

```yaml
model:
  hidden_units: 64        # Hidden dimension (ë…¼ë¬¸: dataset-dependent)
  num_heads: 4            # Attention heads ìˆ˜ (ë…¼ë¬¸: dataset-dependent)
  num_layers: 3           # Transformer blocks ìˆ˜ (ë…¼ë¬¸: 2 for most datasets)
  max_len: 50             # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (ë…¼ë¬¸: 200 for ML-1M)
  dropout_rate: 0.3       # Dropout í™•ë¥  (ë…¼ë¬¸: 0.2~0.5)
  mask_prob: 0.15         # Masking í™•ë¥  (ë…¼ë¬¸: 0.15, BERTì™€ ë™ì¼)
  share_embeddings: true  # Output layerì™€ embedding ê³µìœ  (ë…¼ë¬¸: Yes)
```

**ë…¼ë¬¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
- Hidden units: 64~256 (dataset-dependent)
- Attention heads: 2~8
- Transformer layers: 2~4
- Max sequence length: 200 (ML-1M), 50 (Steam)
- Dropout: 0.2~0.5
- Masking probability: 0.15 (BERT ë…¼ë¬¸ê³¼ ë™ì¼)

**ì¤‘ìš”**: `hidden_units`ëŠ” `num_heads`ë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤.

### í•™ìŠµ ì„¤ì •

```yaml
training:
  num_epochs: 300                  # ìµœëŒ€ epoch ìˆ˜ (ë…¼ë¬¸: early stopping ì‚¬ìš©)
  lr: 0.0015                       # Learning rate (ë…¼ë¬¸: 0.001)
  weight_decay: 0.0                # L2 regularization (ë…¼ë¬¸: ëª…ì‹œ ì•ˆë¨)
  monitor_metric: "val_ndcg@10"    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê¸°ì¤€
  early_stopping: false            # Early stopping (ë…¼ë¬¸: ì‚¬ìš©)
  early_stopping_patience: 20      # Patience
  accelerator: "auto"              # GPU/CPU ìë™ ì„ íƒ
  precision: "32-true"             # ì •ë°€ë„ (ë…¼ë¬¸: 32-bit)
```

## ëª¨ë¸ ì•„í‚¤í…ì²˜

### BERT4Rec êµ¬ì¡° (ë…¼ë¬¸ Figure 2 ê¸°ì¤€)

```
Input Sequence: [itemâ‚, itemâ‚‚, [MASK], itemâ‚„, itemâ‚…]
    â†“
[Item Embedding] + [Position Embedding]
    â†“
    Dropout + LayerNorm
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformer Block 1            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Multi-Head Attention     â”‚   â”‚
â”‚  â”‚ (Bidirectional)          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚          â†“                      â”‚
â”‚    Residual + LayerNorm         â”‚
â”‚          â†“                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Feed-Forward Network     â”‚   â”‚
â”‚  â”‚ (4x expansion, GELU)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚          â†“                      â”‚
â”‚    Residual + LayerNorm         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
    ... (repeat N times)
    â†“
Output Projection (shared with item embedding)
    â†“
Predictions: [logits for all items]
```

### Transformer Block ìƒì„¸

**Multi-Head Attention**:
```
Q, K, V = Linear(x)
Q, K, V = split_heads(Q, K, V)  # [batch, num_heads, seq_len, head_dim]
attn = softmax(QÂ·K^T / âˆšhead_dim)  # Scaled Dot-Product
output = attn Â· V
output = concat_heads(output)
output = Linear(output)
```

**Feed-Forward Network**:
```
FFN(x) = Wâ‚‚(GELU(Wâ‚(x)))
where Wâ‚: hidden â†’ 4*hidden
      Wâ‚‚: 4*hidden â†’ hidden
```

### Special Tokens

- `0`: Padding token
- `1 ~ num_items`: Item indices
- `num_items + 1`: [MASK] token

### Masking Strategy (ë…¼ë¬¸ Section 3.2)

í•™ìŠµ ì‹œ ê° ì•„ì´í…œì€ **15% í™•ë¥ **ë¡œ ë§ˆìŠ¤í‚¹:
- **80%**: `[MASK]` í† í°ìœ¼ë¡œ ëŒ€ì²´
- **10%**: ëœë¤ ì•„ì´í…œìœ¼ë¡œ ëŒ€ì²´
- **10%**: ì›ë³¸ ìœ ì§€

ì¶”ë¡  ì‹œ:
- ë§ˆì§€ë§‰ ìœ„ì¹˜ì— `[MASK]` ì¶”ê°€
- ëª¨ë¸ì´ ë‹¤ìŒ ì•„ì´í…œ ì˜ˆì¸¡

## í‰ê°€ ë©”íŠ¸ë¦­

### HIT@K (Hit Ratio)
```
HIT@K = 1 if ground_truth in top_K else 0
```

### NDCG@K (Normalized Discounted Cumulative Gain)
```
NDCG@K = 1 / logâ‚‚(rank + 2)  if rank < K else 0
```

### MRR (Mean Reciprocal Rank) - ë¯¸êµ¬í˜„
```
MRR = 1 / rank
```

## PyTorch Lightning ê¸°ëŠ¥

### ìë™ ì œê³µë˜ëŠ” ê¸°ëŠ¥

1. **ë¶„ì‚° í•™ìŠµ**: Multi-GPU, TPU ìë™ ì§€ì›
2. **ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬**:
   - `last.ckpt`: ë§ˆì§€ë§‰ epoch
   - `best.ckpt`: ìµœê³  ì„±ëŠ¥ ëª¨ë¸
3. **Early Stopping**: Validation ì„±ëŠ¥ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
4. **ë¡œê¹…**: TensorBoard ìë™ ë¡œê¹…
5. **ì¬í˜„ì„±**: Seed ì„¤ì •ìœ¼ë¡œ ì¬í˜„ ê°€ëŠ¥

### ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ

```python
from src.models.bert4rec import BERT4Rec

# ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ
model = BERT4Rec.load_from_checkpoint("path/to/checkpoint.ckpt")
model.eval()

# ì¶”ë¡ 
predictions = model.predict(
    user_sequences=[[1, 2, 3, 4], [5, 6, 7]],
    topk=10,
    exclude_items=[set([1,2,3,4]), set([5,6,7])]
)
```

## ë…¼ë¬¸ êµ¬í˜„ ì‚¬í•­

### âœ… ë…¼ë¬¸ê³¼ ë™ì¼í•˜ê²Œ êµ¬í˜„ëœ ë¶€ë¶„

| êµ¬ì„± ìš”ì†Œ | ë…¼ë¬¸ ëª…ì„¸ | êµ¬í˜„ ìƒíƒœ |
|-----------|-----------|-----------|
| **Architecture** | Bidirectional Transformer | âœ… êµ¬í˜„ |
| **Attention** | Multi-Head Self-Attention | âœ… êµ¬í˜„ |
| **Scaling** | QÂ·K^T / âˆšd_k (d_k = head_dim) | âœ… êµ¬í˜„ |
| **FFN** | 4x hidden dimension expansion | âœ… êµ¬í˜„ |
| **Activation** | GELU | âœ… êµ¬í˜„ |
| **Normalization** | Layer Normalization | âœ… êµ¬í˜„ |
| **Residual** | Residual Connection | âœ… êµ¬í˜„ |
| **Position Encoding** | Learnable positional embeddings | âœ… êµ¬í˜„ |
| **Masking Strategy** | 15% masking (80% [MASK], 10% random, 10% keep) | âœ… êµ¬í˜„ |
| **Loss** | Cross-Entropy on masked positions | âœ… êµ¬í˜„ |
| **Padding** | Zero padding with padding_idx=0 | âœ… êµ¬í˜„ |

### ğŸ“ ë…¼ë¬¸ê³¼ ë‹¤ë¥´ê±°ë‚˜ ì„ íƒ ê°€ëŠ¥í•œ ë¶€ë¶„

#### 1. Output Layer Weight Sharing

**ë…¼ë¬¸**:
> "we tie the item embedding matrix with the final output projection matrix to reduce parameters"

**êµ¬í˜„**:
```python
share_embeddings: bool = True  # ê¸°ë³¸ê°’: True (ë…¼ë¬¸ê³¼ ë™ì¼)
```

- âœ… **True** (ê¸°ë³¸ê°’): ë…¼ë¬¸ê³¼ ë™ì¼, item embedding ì¬ì‚¬ìš©
- âŒ **False**: ë³„ë„ì˜ Linear layer ì‚¬ìš© (ë” ë§ì€ íŒŒë¼ë¯¸í„°)

**ì„¤ì • ìœ„ì¹˜**: `configs/bert4rec.yaml`
```yaml
model:
  share_embeddings: true  # ê¶Œì¥ (ë…¼ë¬¸ê³¼ ë™ì¼)
```

#### 2. Validation ì „ëµ

**ë…¼ë¬¸**:
> "we randomly select 100 negative items and rank these 101 items"

**êµ¬í˜„**:
- **ì „ì²´ ì•„ì´í…œ ranking** ë°©ì‹ êµ¬í˜„ (ë” ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
- ìƒ˜í”Œë§ ê¸°ë°˜ í‰ê°€ëŠ” ë¯¸êµ¬í˜„

**ì„ íƒ ì´ìœ **:
- ì •í™•í•œ ë©”íŠ¸ë¦­ ì¸¡ì •ì„ ìœ„í•´ ì „ì²´ ranking ì„ íƒ
- í•„ìš”ì‹œ ìƒ˜í”Œë§ ë°©ì‹ ì¶”ê°€ ê°€ëŠ¥

#### 3. Optimizer

**ë…¼ë¬¸**:
> "We use Adam optimizer with learning rate of 0.001"

**êµ¬í˜„**:
```python
optimizer = torch.optim.Adam(
    self.parameters(),
    lr=self.lr,  # ê¸°ë³¸ê°’: 0.0015
    weight_decay=self.weight_decay  # ê¸°ë³¸ê°’: 0.0
)
```

**ì°¨ì´ì **:
- ë…¼ë¬¸ ê¸°ë³¸ê°’: `lr=0.001`
- êµ¬í˜„ ê¸°ë³¸ê°’: `lr=0.0015` (ì‹¤í—˜ì ìœ¼ë¡œ ì¡°ì • ê°€ëŠ¥)

#### 4. Learning Rate Scheduler

**ë…¼ë¬¸**:
> No explicit mention of learning rate scheduling

**êµ¬í˜„**:
- í˜„ì¬ ë¯¸êµ¬í˜„ (constant learning rate)
- Lightningì˜ `configure_optimizers()`ì—ì„œ ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥

**ì¶”ê°€ ë°©ë²•**:
```python
def configure_optimizers(self):
    optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)
    return [optimizer], [scheduler]
```

#### 5. Dropout ìœ„ì¹˜

**ë…¼ë¬¸**:
> "we apply dropout on all intermediate layers including the embedding layer"

**êµ¬í˜„**:
- âœ… Embedding layer ì´í›„
- âœ… Attention distribution
- âœ… Feed-forward network
- âœ… Residual connection ì´í›„

**ëª¨ë‘ ë…¼ë¬¸ê³¼ ë™ì¼**

#### 6. Attention Mask

**ë…¼ë¬¸**:
> "bidirectional model... each position can attend to all positions"

**êµ¬í˜„**:
```python
# Paddingë§Œ ë§ˆìŠ¤í‚¹ (bidirectional attention ìœ ì§€)
mask = (log_seqs > 0).unsqueeze(1).unsqueeze(2)
mask = mask.expand(-1, -1, seq_len, -1)  # [batch, 1, seq_len, seq_len]
```

**ë…¼ë¬¸ê³¼ ë™ì¼**: íŒ¨ë”© ìœ„ì¹˜ë§Œ ë§ˆìŠ¤í‚¹, ëª¨ë“  ìœ íš¨í•œ ìœ„ì¹˜ëŠ” ì„œë¡œ ë³¼ ìˆ˜ ìˆìŒ

#### 7. í‰ê°€ ë©”íŠ¸ë¦­

**ë…¼ë¬¸**:
> "HIT@K, NDCG@K, MRR"

**êµ¬í˜„**:
- âœ… HIT@10
- âœ… NDCG@10
- âŒ MRR (ë¯¸êµ¬í˜„)

**MRR ì¶”ê°€ ê°€ëŠ¥**:
```python
# validation_stepì— ì¶”ê°€
mrr = 1.0 / (rank + 1)  # rankëŠ” 0-based
self.log('val_mrr', mrr, ...)
```

## ì„±ëŠ¥ ìµœì í™” íŒ

### 1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ

```yaml
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
data:
  batch_size: 64

# Mixed precision ì‚¬ìš©
training:
  precision: "16-mixed"
```

### 2. í•™ìŠµ ì†ë„ í–¥ìƒ

```yaml
# DataLoader workers ì¦ê°€
data:
  num_workers: 8

# ê²€ì¦ ë¹ˆë„ ì¤„ì´ê¸° (2 epochë§ˆë‹¤)
training:
  val_check_interval: 2.0
```

### 3. ë°ì´í„°ì…‹ë³„ ê¶Œì¥ ì„¤ì •

**ì‘ì€ ë°ì´í„°ì…‹** (< 10K users):
```yaml
model:
  hidden_units: 32
  num_heads: 2
  num_layers: 2
  max_len: 30
```

**ì¤‘ê°„ ë°ì´í„°ì…‹** (10K ~ 100K users):
```yaml
model:
  hidden_units: 64
  num_heads: 4
  num_layers: 3
  max_len: 50
```

**í° ë°ì´í„°ì…‹** (> 100K users):
```yaml
model:
  hidden_units: 128
  num_heads: 8
  num_layers: 4
  max_len: 200
```

## ë…¼ë¬¸ ì¬í˜„ì„ ìœ„í•œ ì„¤ì •

### MovieLens-1M (ë…¼ë¬¸ Table 2)

```yaml
model:
  hidden_units: 256
  num_heads: 4
  num_layers: 2
  max_len: 200
  dropout_rate: 0.2
  mask_prob: 0.15

training:
  lr: 0.001
  num_epochs: 200
```

### Steam (ë…¼ë¬¸ Table 2)

```yaml
model:
  hidden_units: 256
  num_heads: 4
  num_layers: 2
  max_len: 50
  dropout_rate: 0.5
  mask_prob: 0.15

training:
  lr: 0.001
  num_epochs: 200
```

## ë¬¸ì œ í•´ê²°

### Import Error

```bash
# PYTHONPATH ì„¤ì •
export PYTHONPATH=/data/ephemeral/home/juik/lightning:$PYTHONPATH
```

### CUDA Out of Memory

```yaml
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
data:
  batch_size: 64

# ì‹œí€€ìŠ¤ ê¸¸ì´ ì¤„ì´ê¸°
model:
  max_len: 30

# Mixed precision ì‚¬ìš©
training:
  precision: "16-mixed"
```

### í•™ìŠµì´ ëŠë¦° ê²½ìš°

```yaml
# 1. Workers ì¦ê°€
data:
  num_workers: 8

# 2. ê²€ì¦ ë¹ˆë„ ì¤„ì´ê¸°
training:
  val_check_interval: 2.0

# 3. Pin memory í™œì„±í™” (DataLoaderì—ì„œ ìë™)
```

### Validation ì„±ëŠ¥ì´ ë‚®ì€ ê²½ìš°

1. **Masking í™•ë¥  ì¡°ì •**:
   ```yaml
   model:
     mask_prob: 0.2  # 0.15ì—ì„œ ì¦ê°€
   ```

2. **Dropout ì¤„ì´ê¸°**:
   ```yaml
   model:
     dropout_rate: 0.2  # 0.3ì—ì„œ ê°ì†Œ
   ```

3. **ëª¨ë¸ í¬ê¸° ì¦ê°€**:
   ```yaml
   model:
     hidden_units: 128
     num_layers: 4
   ```

## êµ¬í˜„ ì„¸ë¶€ ì‚¬í•­

### Weight Initialization (ë…¼ë¬¸ ëª…ì‹œ ì•ˆë¨)

```python
# Normal distribution (mean=0, std=0.02)
nn.init.normal_(module.weight, mean=0.0, std=0.02)

# Padding embeddingì€ 0ìœ¼ë¡œ ì´ˆê¸°í™”
if module.padding_idx is not None:
    nn.init.zeros_(module.weight[module.padding_idx])
```

### Loss Computation

```python
# Cross-entropy on masked positions only
criterion = nn.CrossEntropyLoss(ignore_index=0)

# ignore_index=0: íŒ¨ë”© ë° ë¹„ë§ˆìŠ¤í‚¹ ìœ„ì¹˜ ë¬´ì‹œ
```

### Attention Mask Shape

```python
# Input: [batch, seq_len]
# Mask: [batch, 1, seq_len, seq_len]
#
# mask[b, 0, i, j] = 1 if position j is valid (not padding)
#                  = 0 if position j is padding
```

## ì°¸ê³  ìë£Œ

- [BERT4Rec ë…¼ë¬¸](https://arxiv.org/abs/1904.06690)
- [BERT ì›ë³¸ ë…¼ë¬¸](https://arxiv.org/abs/1810.04805)
- [PyTorch Lightning ë¬¸ì„œ](https://lightning.ai/docs/pytorch/stable/)
- [Hydra ë¬¸ì„œ](https://hydra.cc/)

## ë¼ì´ì„ ìŠ¤

MIT License

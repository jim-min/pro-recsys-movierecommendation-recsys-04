# PyTorch Lightning Checkpoint ìë£Œêµ¬ì¡°

PyTorch Lightningì˜ `.ckpt` íŒŒì¼ì€ PyTorchì˜ pickle í¬ë§·ìœ¼ë¡œ ì €ì¥ë˜ë©°, ëª¨ë¸ ê°€ì¤‘ì¹˜ë¿ë§Œ ì•„ë‹ˆë¼ í•™ìŠµ ìƒíƒœë¥¼ ì™„ì „íˆ ì¬í˜„í•˜ê¸° ìœ„í•œ ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

## íŒŒì¼ êµ¬ì¡°

```
checkpoint_file.ckpt (PyTorch pickle format, ~101MB)
â”‚
â”œâ”€ ğŸ“‹ hyper_parameters          # ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
â”‚   â”œâ”€ num_items: 6807
â”‚   â”œâ”€ hidden_dims: [600, 200]
â”‚   â”œâ”€ dropout: 0.5
â”‚   â”œâ”€ lr: 0.001
â”‚   â”œâ”€ weight_decay: 0.0
â”‚   â”œâ”€ kl_max_weight: 0.2
â”‚   â””â”€ kl_anneal_steps: 20000
â”‚
â”œâ”€ ğŸ”§ state_dict               # ëª¨ë¸ ê°€ì¤‘ì¹˜ (12ê°œ í…ì„œ)
â”‚   â”œâ”€ encoder.0.weight        [600, 6807]   # Input â†’ Hidden1
â”‚   â”œâ”€ encoder.0.bias          [600]
â”‚   â”œâ”€ encoder.2.weight        [200, 600]    # Hidden1 â†’ Hidden2
â”‚   â”œâ”€ encoder.2.bias          [200]
â”‚   â”œâ”€ mu.weight               [200, 200]    # Hidden2 â†’ Î¼
â”‚   â”œâ”€ mu.bias                 [200]
â”‚   â”œâ”€ logvar.weight           [200, 200]    # Hidden2 â†’ log(ÏƒÂ²)
â”‚   â”œâ”€ logvar.bias             [200]
â”‚   â”œâ”€ decoder.0.weight        [600, 200]    # Latent â†’ Hidden1
â”‚   â”œâ”€ decoder.0.bias          [600]
â”‚   â”œâ”€ decoder.2.weight        [6807, 600]   # Hidden1 â†’ Output
â”‚   â””â”€ decoder.2.bias          [6807]
â”‚
â”œâ”€ âš™ï¸  optimizer_states         # Optimizer ìƒíƒœ (list[1])
â”‚   â””â”€ [0] Adam
â”‚       â”œâ”€ state               # ê° íŒŒë¼ë¯¸í„°ë³„ momentum ë“±
â”‚       â”‚   â””â”€ {12ê°œ íŒŒë¼ë¯¸í„°ì˜ ìƒíƒœ}
â”‚       â””â”€ param_groups        # Optimizer ì„¤ì •
â”‚           â”œâ”€ lr: 1.5625e-05  (í˜„ì¬ learning rate)
â”‚           â”œâ”€ betas: (0.9, 0.999)
â”‚           â”œâ”€ weight_decay: 0.0
â”‚           â””â”€ ...
â”‚
â”œâ”€ ğŸ”„ lr_schedulers            # Learning Rate Scheduler (list[1])
â”‚   â””â”€ [0] scheduler ìƒíƒœ
â”‚
â”œâ”€ ğŸ“Š callbacks                # Callback ë©”íƒ€ë°ì´í„°
â”‚   â”œâ”€ EarlyStopping
â”‚   â”‚   â”œâ”€ best_score: 1064.78
â”‚   â”‚   â”œâ”€ wait_count: 0
â”‚   â”‚   â””â”€ patience: 20
â”‚   â””â”€ ModelCheckpoint
â”‚       â”œâ”€ best_model_path: "...epoch=435-val_loss=1064.7800.ckpt"
â”‚       â”œâ”€ best_model_score: 1064.78
â”‚       â”œâ”€ last_model_path: ".../last.ckpt"
â”‚       â”œâ”€ kth_best_model_path: "..."
â”‚       â””â”€ best_k_models: {path: score}
â”‚
â”œâ”€ ğŸ” loops                    # Training loop ìƒíƒœ
â”‚   â””â”€ epoch/batch ì§„í–‰ ìƒíƒœ
â”‚
â””â”€ ğŸ¯ ë©”íƒ€ë°ì´í„°
    â”œâ”€ epoch: 435              # ì €ì¥ ì‹œì ì˜ epoch
    â”œâ”€ global_step: 106820     # ì „ì²´ step ìˆ˜
    â””â”€ pytorch-lightning_version: "2.6.0"
```

## ì£¼ìš” êµ¬ì„±ìš”ì†Œ ì„¤ëª…

### 1. `hyper_parameters` (dict)

ëª¨ë¸ ì´ˆê¸°í™”ì— í•„ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì´ ì €ì¥ë©ë‹ˆë‹¤.

```python
{
    'num_items': 6807,
    'hidden_dims': [600, 200],
    'dropout': 0.5,
    'lr': 0.001,
    'weight_decay': 0.0,
    'kl_max_weight': 0.2,
    'kl_anneal_steps': 20000
}
```

**ì¤‘ìš”**:
- `load_from_checkpoint()` í˜¸ì¶œ ì‹œ ì´ ê°’ë“¤ì´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤
- íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬í•˜ë©´ ì €ì¥ëœ ê°’ì„ ì˜¤ë²„ë¼ì´ë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- MultiVAEëŠ” `num_items`ê°€ ëª¨ë¸ êµ¬ì¡°ë¥¼ ê²°ì •í•˜ë¯€ë¡œ ë°˜ë“œì‹œ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤

### 2. `state_dict` (dict)

ì‹¤ì œ í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜(í…ì„œ)ë“¤ì´ ì €ì¥ë©ë‹ˆë‹¤.

**MultiVAE êµ¬ì¡°**:
- **Encoder**: items(6807) â†’ 600 â†’ 200
  - `encoder.0.*`: ì²« ë²ˆì§¸ fully-connected layer
  - `encoder.2.*`: ë‘ ë²ˆì§¸ fully-connected layer

- **Latent Variables**: 200 â†’ 200
  - `mu.*`: í‰ê· (Î¼) ê³„ì‚° ë ˆì´ì–´
  - `logvar.*`: ë¡œê·¸ë¶„ì‚°(log ÏƒÂ²) ê³„ì‚° ë ˆì´ì–´

- **Decoder**: 200 â†’ 600 â†’ items(6807)
  - `decoder.0.*`: ì²« ë²ˆì§¸ fully-connected layer
  - `decoder.2.*`: ì¶œë ¥ ë ˆì´ì–´

ì´ **12ê°œì˜ íŒŒë¼ë¯¸í„° í…ì„œ**ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

### 3. `optimizer_states` (list)

Adam optimizerì˜ ë‚´ë¶€ ìƒíƒœë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

```python
[
    {
        'state': {
            # ê° íŒŒë¼ë¯¸í„°ë³„ momentum, variance ë“±
            0: {'step': ..., 'exp_avg': ..., 'exp_avg_sq': ...},
            1: {...},
            ...
        },
        'param_groups': [
            {
                'lr': 1.5625e-05,  # í˜„ì¬ í•™ìŠµë¥ 
                'betas': (0.9, 0.999),
                'eps': 1e-08,
                'weight_decay': 0.0,
                ...
            }
        ]
    }
]
```

**ìš©ë„**:
- í•™ìŠµ ì¬ê°œ(resume training) ì‹œ í•„ìš”
- Inference ì‹œì—ëŠ” ë¶ˆí•„ìš”

### 4. `callbacks` (dict)

PyTorch Lightning callbackë“¤ì˜ ìƒíƒœë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

#### EarlyStopping
```python
{
    'best_score': tensor(1064.7800),
    'wait_count': 0,
    'patience': 20,
    'stopped_epoch': 0
}
```

#### ModelCheckpoint
```python
{
    'best_model_path': '/path/to/checkpoints/multi-vae-epoch=435-val_loss=1064.7800.ckpt',
    'best_model_score': tensor(1064.7800),
    'last_model_path': '/path/to/checkpoints/last.ckpt',
    'best_k_models': {
        '/path/to/checkpoints/multi-vae-epoch=435-val_loss=1064.7800.ckpt': tensor(1064.7800)
    }
}
```

**ì£¼ì˜ì‚¬í•­**:
- `last_model_path`ê°€ checkpointì— ì €ì¥ë˜ì–´ ìˆì–´ë„ ì‹¤ì œ íŒŒì¼ì´ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- `save_last=False` ì„¤ì • ì‹œ `last.ckpt`ê°€ ìƒì„±ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤

### 5. ë©”íƒ€ë°ì´í„°

```python
{
    'epoch': 435,                          # ì €ì¥ ì‹œì  epoch
    'global_step': 106820,                 # ì „ì²´ training step
    'pytorch-lightning_version': '2.6.0',  # Lightning ë²„ì „
    'hparams_name': 'kwargs',
    'loops': {...}                         # Training loop ìƒíƒœ
}
```

## Checkpoint ë¡œë”© ë°©ë²•

### ê¸°ë³¸ ë¡œë”© (PyTorch 2.6+)

```python
import torch

# PyTorch 2.6ë¶€í„°ëŠ” weights_only=False ëª…ì‹œ í•„ìš”
checkpoint = torch.load(
    'checkpoint.ckpt',
    map_location='cpu',
    weights_only=False  # OmegaConf ë“± ì»¤ìŠ¤í…€ ê°ì²´ í¬í•¨
)

# ì£¼ìš” í‚¤ í™•ì¸
print(checkpoint.keys())
# dict_keys(['epoch', 'global_step', 'pytorch-lightning_version',
#            'state_dict', 'loops', 'callbacks', 'optimizer_states',
#            'lr_schedulers', 'hparams_name', 'hyper_parameters'])
```

### Lightning ëª¨ë¸ë¡œ ë¡œë”©

```python
from src.models.multi_vae import MultiVAE

# ë°©ë²• 1: checkpointì˜ hyper_parameters ì‚¬ìš©
model = MultiVAE.load_from_checkpoint(
    'checkpoint.ckpt',
    num_items=6807,  # í•„ìˆ˜! (ëª¨ë¸ êµ¬ì¡° ê²°ì •)
    weights_only=False
)

# ë°©ë²• 2: ëª¨ë“  hyperparameter ëª…ì‹œ
model = MultiVAE.load_from_checkpoint(
    'checkpoint.ckpt',
    num_items=6807,
    hidden_dims=[600, 200],
    dropout=0.5,
    lr=0.001,
    weights_only=False
)
```

### BERT4Rec vs MultiVAE ì°¨ì´ì 

**BERT4Rec**:
```python
# num_itemsê°€ ëª¨ë¸ êµ¬ì¡°ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ
model = BERT4Rec.load_from_checkpoint(checkpoint_path)
```

**MultiVAE**:
```python
# num_itemsê°€ ì²« ë²ˆì§¸ ë ˆì´ì–´ í¬ê¸°ë¥¼ ê²°ì •
# ë°˜ë“œì‹œ ëª…ì‹œí•´ì•¼ í•¨!
model = MultiVAE.load_from_checkpoint(
    checkpoint_path,
    num_items=datamodule.num_items,
    weights_only=False
)
```

**ì´ìœ **:
- BERT4Rec: `num_items`ëŠ” ìµœì¢… prediction headì—ì„œë§Œ ì‚¬ìš© (ë™ì  ì²˜ë¦¬ ê°€ëŠ¥)
- MultiVAE: `num_items`ê°€ encoderì˜ ì²« ë²ˆì§¸ layer í¬ê¸° ê²°ì • (ê³ ì •ë¨)

## íŒŒì¼ í¬ê¸°

- **MultiVAE checkpoint**: ~101MB
  - state_dictê°€ ëŒ€ë¶€ë¶„ì˜ ìš©ëŸ‰ ì°¨ì§€
  - ê°€ì¥ í° í…ì„œ: `encoder.0.weight` [600, 6807]ì™€ `decoder.2.weight` [6807, 600]

## ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì„¤ì •

### train_multi_vae.py
```python
checkpoint_callback = ModelCheckpoint(
    dirpath=checkpoint_dir,
    filename="multi-vae-{epoch:02d}-{val_loss:.4f}",
    monitor="val_loss",
    mode="min",
    save_top_k=1,      # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ 1ê°œë§Œ ì €ì¥
    save_last=False,   # âš ï¸ last.ckpt ì €ì¥ ì•ˆ í•¨
    verbose=True,
)
```

**ì£¼ì˜**: `save_last=False`ë¡œ ì„¤ì •ë˜ì–´ ìˆì–´ `last.ckpt`ê°€ ìƒì„±ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

## ë¬¸ì œ í•´ê²°

### 1. "FileNotFoundError: last.ckpt"

**ì›ì¸**: `save_last=False` ì„¤ì •

**í•´ê²°ì±…**:
```python
# Option 1: save_last=Trueë¡œ ë³€ê²½
checkpoint_callback = ModelCheckpoint(..., save_last=True)

# Option 2: best model ì‚¬ìš©
checkpoint_path = get_latest_checkpoint(checkpoint_dir)  # best model ë°˜í™˜
```

### 2. "num_items mismatch"

**ì›ì¸**: checkpointì˜ `num_items`ì™€ í˜„ì¬ ë°ì´í„°ì˜ `num_items` ë¶ˆì¼ì¹˜

**í•´ê²°ì±…**:
```python
model = MultiVAE.load_from_checkpoint(
    checkpoint_path,
    num_items=datamodule.num_items,  # í˜„ì¬ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë²„ë¼ì´ë“œ
    weights_only=False
)
```

### 3. "Weights only load failed"

**ì›ì¸**: PyTorch 2.6+ ë²„ì „ì—ì„œ `weights_only=True`ê°€ ê¸°ë³¸ê°’

**í•´ê²°ì±…**:
```python
# torch.load ì‚¬ìš© ì‹œ
checkpoint = torch.load(path, weights_only=False)

# load_from_checkpoint ì‚¬ìš© ì‹œ
model = MultiVAE.load_from_checkpoint(path, weights_only=False)
```

## ì°¸ê³  ìë£Œ

- [PyTorch Lightning Checkpointing](https://lightning.ai/docs/pytorch/stable/common/checkpointing.html)
- [torch.load Documentation](https://pytorch.org/docs/stable/generated/torch.load.html)
- [ModelCheckpoint API](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.ModelCheckpoint.html)
